{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scratch_CNN_masses_vs_calc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarshaGomez/CNN-Medical-Imaging-Analysis/blob/main/Code/Scratch_CNN_masses_vs_calc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPQ8gkXb0NY0"
      },
      "source": [
        "# **Scratch CNN**\r\n",
        "---\r\n",
        "Classification model for discriminating between 2 classes: **masses and calcification**. *Ad-hoc CNN architecture*.\r\n",
        "\r\n",
        "**Students:**   *A. Schiavo - M. GÃ³mez - M. Daole*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB6hExesaEvW"
      },
      "source": [
        "## Data Loading\r\n",
        "This can be easily done with the Python data manipulation. Modern deep learning provides a very powerful framework for supervised learning, we introduce on this step the convolutional network for scaling to large images.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95hM4MvWjwfO",
        "outputId": "a63204cd-4673-41f6-b18c-3a4fe076ca8f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iduaD281krFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2e82df-d296-436a-92c2-e2ce63fedf84"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import ast #Abstract Syntax Trees\r\n",
        "import os \r\n",
        "import gc # Garbage Collector\r\n",
        "\r\n",
        "from tensorflow.keras import backend as K # Useful to free GPU and memory\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from keras import layers\r\n",
        "from keras import optimizers \r\n",
        "from keras import models\r\n",
        "from keras import regularizers\r\n",
        "\r\n",
        "BATCH_SIZE = 20\r\n",
        "EPOCHS = 100\r\n",
        "\r\n",
        "base_path = \"/content/gdrive/My Drive/Colab_Notebooks/CIDL/DL Project\"\r\n",
        "train_img_path = os.path.join(base_path, \"numpy data/train_tensor.npy\")\r\n",
        "train_label_path = os.path.join(base_path, \"numpy data/train_labels.npy\")\r\n",
        "test_img_path = os.path.join(base_path, \"numpy data/public_test_tensor.npy\")\r\n",
        "test_label_path = os.path.join(base_path, \"numpy data/public_test_labels.npy\")\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HEhhQy4Sr_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43676fe8-d044-43da-d946-3fa6b0d443ef"
      },
      "source": [
        "# Custom Callback To Include in Callbacks List At Training Time\r\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek9JMlmD6GsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0566aba-23a9-4732-9913-dcbda5df89c8"
      },
      "source": [
        "# Load Arrays from Numpy Files\r\n",
        "def load_training():\r\n",
        "  train_images = np.load(train_img_path)\r\n",
        "  train_labels = np.load(train_label_path)\r\n",
        "  test_images = np.load(test_img_path)\r\n",
        "  test_labels = np.load(test_label_path)\r\n",
        "\r\n",
        "  return train_images, train_labels, test_images, test_labels\r\n",
        "\r\n",
        "# Remove baseline samples\r\n",
        "def remove_baseline(tensor): \r\n",
        "  max_ind = int(len(tensor)/2)\r\n",
        "  indexes = [2*i + 1 for i in range(0, max_ind)]\r\n",
        "\r\n",
        "  return tensor[indexes]\r\n",
        "\r\n",
        "# Interchange the dataset index\r\n",
        "def shuffle_dataset(x, y):\r\n",
        "  indices = tf.range(start=0, limit=tf.shape(x)[0], dtype=tf.int32)\r\n",
        "  shuffled_indices = tf.random.shuffle(indices)\r\n",
        "\r\n",
        "  x = tf.gather(x, shuffled_indices)\r\n",
        "  y = tf.gather(y, shuffled_indices)\r\n",
        "\r\n",
        "  x = x.numpy()\r\n",
        "  y = y.numpy()\r\n",
        "\r\n",
        "  return x, y\r\n",
        "\r\n",
        "# Unify masses and calcifications \r\n",
        "def labels_mapping(labels):\r\n",
        "  labels_local = np.zeros(shape=labels.shape, dtype=\"float32\")\r\n",
        "  idx = 0\r\n",
        "  for label in labels:\r\n",
        "    # Masses\r\n",
        "    if label == 1 or label == 2:\r\n",
        "      labels_local[idx] = 0\r\n",
        "    # Calcifications\r\n",
        "    else:\r\n",
        "      labels_local[idx] = 1\r\n",
        "    idx += 1\r\n",
        "\r\n",
        "  return labels_local\r\n",
        "\r\n",
        "# Visualization Data Histogram\r\n",
        "def plot(history):\r\n",
        "  acc = history.history['acc']\r\n",
        "  val_acc = history.history['val_acc']\r\n",
        "  loss = history.history['loss']\r\n",
        "  val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "  epochs = range(len(acc))\r\n",
        "\r\n",
        "  plt.plot(epochs, acc, 'bo', label='Training accuracy')\r\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\r\n",
        "  plt.title('Training and validation accuracy')\r\n",
        "  plt.legend()\r\n",
        "\r\n",
        "  plt.figure()\r\n",
        "\r\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\r\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\r\n",
        "  plt.title('Training and validation loss')\r\n",
        "  plt.legend()\r\n",
        "\r\n",
        "  return plt\r\n",
        "\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gowt9JUQ6sdj",
        "outputId": "c41c12b5-296b-43ef-89bb-3c2318463e90"
      },
      "source": [
        "# Get images and labels (test, train)\r\n",
        "train_images, train_labels, test_images, test_labels = load_training()\r\n",
        "\r\n",
        "# Get abnormalities only \r\n",
        "train_images = remove_baseline(train_images)\r\n",
        "train_labels = remove_baseline(train_labels)\r\n",
        "test_images = remove_baseline(test_images)\r\n",
        "test_labels = remove_baseline(test_labels)\r\n",
        "\r\n",
        "# Mapping labels with standard index\r\n",
        "train_labels = labels_mapping(train_labels)\r\n",
        "test_labels = labels_mapping(test_labels)\r\n",
        "\r\n",
        "# Suffle index (Previous dataset is ordered by index)\r\n",
        "train_images, train_labels = shuffle_dataset(train_images, train_labels)\r\n",
        "\r\n",
        "print(\"Train shape: \", train_images.shape)\r\n",
        "print(\"Test shape: \", test_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape:  (2676, 150, 150)\n",
            "Test shape:  (336, 150, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "nI7cnT2gDyf_",
        "outputId": "d83847af-711c-4e18-af68-47c5481dcbbd"
      },
      "source": [
        "# Check the value range and the distribution\r\n",
        "plt.hist(train_images[0]) \r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrrJFfg2e4fL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660b858c-a5e9-441e-b226-b445ad337514"
      },
      "source": [
        "# Prepare the data with the expected format\r\n",
        "train_images = train_images.reshape(train_images.shape + (1,))\r\n",
        "test_images = test_images.reshape(test_images.shape + (1,))\r\n",
        "\r\n",
        "print(\"Train shape: \", train_images.shape)\r\n",
        "print(\"Test shape: \", test_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape:  (2676, 150, 150, 1)\n",
            "Test shape:  (336, 150, 150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t4iLd0ExZho"
      },
      "source": [
        "## Data preprocessing\r\n",
        "\r\n",
        "Computer vision usually requires relatively little of this kind of preprocessing. The images should be standardized, formatting images to have the same scale is the only kind of preprocessing that is strictly necessary. As optional, we add dataset augmentation because is an excellent way to reduce the generalization error of most computer vision models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfMviUsBn-gX",
        "outputId": "68e65201-1fa8-4bb6-f552-1b3f6575f05a"
      },
      "source": [
        "# Split dataset into training and validation set 70-30\r\n",
        "train_images_split = train_images[:int(0.7*len(train_images))]\r\n",
        "valid_images_split = train_images[int(0.7*len(train_images)):]\r\n",
        "train_labels_split = train_labels[:int(0.7*len(train_labels))]\r\n",
        "valid_labels_split = train_labels[int(0.7*len(train_labels)):]\r\n",
        "\r\n",
        "print(train_images_split.shape)\r\n",
        "print(valid_images_split.shape)                                       "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1873, 150, 150, 1)\n",
            "(803, 150, 150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAimqVVOhaH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f931ad-11fd-466f-c97e-a906995e7776"
      },
      "source": [
        "# Verify values range: \r\n",
        "# Getting max value \r\n",
        "max = max([np.max(image) for image in train_images]) # max is 65535 \r\n",
        "\r\n",
        "# Getting min value \r\n",
        "min = min([np.min(image) for image in train_images]) # min is 0\r\n",
        "\r\n",
        "print(\"Original tensor are of type \", train_images[0].dtype, \" with values in the range [\", min,\",\", max, \"]\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tensor are of type  uint16  with values in the range [ 0 , 65535 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm9x-21nzEoE",
        "outputId": "46b28b97-60e0-447c-b15e-a15a23441058"
      },
      "source": [
        "# All images will be rescaled by 1./65535 (max value range)\r\n",
        "train_datagen = ImageDataGenerator(rescale=1./65535)\r\n",
        "valid_datagen = ImageDataGenerator(rescale=1./65535)\r\n",
        "test_datagen = ImageDataGenerator(rescale=1./65535) \r\n",
        "\r\n",
        "for batch, labels_batch in train_datagen.flow(train_images, train_labels, batch_size=BATCH_SIZE):\r\n",
        "  print(batch.shape)\r\n",
        "  print(labels_batch.shape)\r\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 150, 150, 1)\n",
            "(20,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi0ZJVf1vA31"
      },
      "source": [
        "### Defining CNN \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwQTeDPEvKql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0d791c-77f8-4221-f8b8-005120152035"
      },
      "source": [
        "def build_model(loss_function, eval_metric):\r\n",
        "  model = models.Sequential()\r\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)))\r\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\r\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\r\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "  model.add(layers.Flatten())\r\n",
        "  #model.add(layers.Dense(512, activation='relu'))\r\n",
        "  #model.add(layers.Dropout(0.5))\r\n",
        "  model.add(layers.Dense(512, kernel_regularizer=regularizers.l2(0.001), activation=\"relu\"))\r\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "  model.compile(loss=loss_function,\r\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4), # lr = 0.0001\r\n",
        "              metrics=[\"acc\"]) \r\n",
        "  \r\n",
        "  return model \r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZIemXxawWLm"
      },
      "source": [
        "### CNN Compilation\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iumGmf1whk1",
        "outputId": "c287db7a-84e1-4438-fbfe-be3243baf314"
      },
      "source": [
        "model = build_model(\"binary_crossentropy\", \"acc\")\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 148, 148, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 3,452,545\n",
            "Trainable params: 3,452,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-V8cw96y_3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f973c3e1-82c7-4008-ea66-7f7cd479101e"
      },
      "source": [
        "history = model.fit(train_datagen.flow(train_images_split,\r\n",
        "                                       train_labels_split,\r\n",
        "                                       batch_size=BATCH_SIZE,\r\n",
        "                                       shuffle=False),\r\n",
        "                    steps_per_epoch=len(train_images_split) // BATCH_SIZE, \r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    validation_data=valid_datagen.flow(valid_images_split,\r\n",
        "                                       valid_labels_split,\r\n",
        "                                       batch_size=BATCH_SIZE,\r\n",
        "                                       shuffle=False),\r\n",
        "                    validation_steps=len(valid_labels_split) // BATCH_SIZE,\r\n",
        "                    callbacks=[GarbageCollectorCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "93/93 [==============================] - 10s 26ms/step - loss: 1.4299 - acc: 0.5381 - val_loss: 1.0436 - val_acc: 0.5512\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.9752 - acc: 0.5685 - val_loss: 0.8028 - val_acc: 0.7225\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.7698 - acc: 0.6484 - val_loss: 0.6462 - val_acc: 0.7575\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.6334 - acc: 0.7555 - val_loss: 0.5706 - val_acc: 0.8188\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.6008 - acc: 0.7790 - val_loss: 0.6677 - val_acc: 0.6488\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.5481 - acc: 0.7891 - val_loss: 0.5457 - val_acc: 0.8250\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.5375 - acc: 0.8034 - val_loss: 0.5149 - val_acc: 0.8288\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.5003 - acc: 0.8111 - val_loss: 0.4958 - val_acc: 0.8125\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4960 - acc: 0.8120 - val_loss: 0.7507 - val_acc: 0.6325\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.5158 - acc: 0.7953 - val_loss: 0.4875 - val_acc: 0.8375\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4805 - acc: 0.8343 - val_loss: 0.4846 - val_acc: 0.8125\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4574 - acc: 0.8303 - val_loss: 0.5284 - val_acc: 0.8037\n",
            "Epoch 13/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4380 - acc: 0.8363 - val_loss: 0.4536 - val_acc: 0.8400\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4532 - acc: 0.8306 - val_loss: 0.4665 - val_acc: 0.8025\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4441 - acc: 0.8377 - val_loss: 0.5022 - val_acc: 0.7837\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4252 - acc: 0.8328 - val_loss: 0.4370 - val_acc: 0.8375\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4319 - acc: 0.8361 - val_loss: 0.5957 - val_acc: 0.7688\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4244 - acc: 0.8484 - val_loss: 0.5123 - val_acc: 0.7688\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4157 - acc: 0.8397 - val_loss: 0.4341 - val_acc: 0.8462\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3950 - acc: 0.8578 - val_loss: 0.4318 - val_acc: 0.8338\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4026 - acc: 0.8481 - val_loss: 0.4503 - val_acc: 0.8250\n",
            "Epoch 22/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3856 - acc: 0.8573 - val_loss: 0.4199 - val_acc: 0.8413\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3859 - acc: 0.8636 - val_loss: 0.3981 - val_acc: 0.8562\n",
            "Epoch 24/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.4121 - acc: 0.8541 - val_loss: 0.3746 - val_acc: 0.8562\n",
            "Epoch 25/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.4000 - acc: 0.8508 - val_loss: 0.3875 - val_acc: 0.8587\n",
            "Epoch 26/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3701 - acc: 0.8585 - val_loss: 0.3687 - val_acc: 0.8775\n",
            "Epoch 27/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3526 - acc: 0.8658 - val_loss: 0.3836 - val_acc: 0.8712\n",
            "Epoch 28/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3817 - acc: 0.8521 - val_loss: 0.3545 - val_acc: 0.8750\n",
            "Epoch 29/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3579 - acc: 0.8712 - val_loss: 0.3860 - val_acc: 0.8388\n",
            "Epoch 30/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3583 - acc: 0.8768 - val_loss: 0.3534 - val_acc: 0.8675\n",
            "Epoch 31/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3418 - acc: 0.8745 - val_loss: 0.3628 - val_acc: 0.8562\n",
            "Epoch 32/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3530 - acc: 0.8686 - val_loss: 0.4096 - val_acc: 0.8263\n",
            "Epoch 33/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3352 - acc: 0.8766 - val_loss: 0.3451 - val_acc: 0.8750\n",
            "Epoch 34/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3748 - acc: 0.8508 - val_loss: 0.3886 - val_acc: 0.8550\n",
            "Epoch 35/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3326 - acc: 0.8835 - val_loss: 0.3490 - val_acc: 0.8587\n",
            "Epoch 36/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.3217 - acc: 0.8707 - val_loss: 0.3702 - val_acc: 0.8625\n",
            "Epoch 37/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3217 - acc: 0.8863 - val_loss: 0.3405 - val_acc: 0.8775\n",
            "Epoch 38/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3311 - acc: 0.8632 - val_loss: 0.3513 - val_acc: 0.8612\n",
            "Epoch 39/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3006 - acc: 0.8949 - val_loss: 0.4779 - val_acc: 0.8112\n",
            "Epoch 40/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3115 - acc: 0.8920 - val_loss: 0.3488 - val_acc: 0.8637\n",
            "Epoch 41/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3185 - acc: 0.8850 - val_loss: 0.3761 - val_acc: 0.8450\n",
            "Epoch 42/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3029 - acc: 0.8822 - val_loss: 0.3456 - val_acc: 0.8800\n",
            "Epoch 43/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3062 - acc: 0.8889 - val_loss: 0.3704 - val_acc: 0.8562\n",
            "Epoch 44/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2969 - acc: 0.8977 - val_loss: 0.5742 - val_acc: 0.7487\n",
            "Epoch 45/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3028 - acc: 0.8909 - val_loss: 0.3708 - val_acc: 0.8813\n",
            "Epoch 46/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3135 - acc: 0.8850 - val_loss: 0.3990 - val_acc: 0.8625\n",
            "Epoch 47/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2802 - acc: 0.9024 - val_loss: 0.3335 - val_acc: 0.8775\n",
            "Epoch 48/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.3182 - acc: 0.8864 - val_loss: 0.4353 - val_acc: 0.8313\n",
            "Epoch 49/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2592 - acc: 0.9104 - val_loss: 0.4333 - val_acc: 0.8388\n",
            "Epoch 50/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2678 - acc: 0.9159 - val_loss: 0.3322 - val_acc: 0.8775\n",
            "Epoch 51/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2669 - acc: 0.9142 - val_loss: 0.4331 - val_acc: 0.8525\n",
            "Epoch 52/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2701 - acc: 0.8985 - val_loss: 0.3341 - val_acc: 0.8775\n",
            "Epoch 53/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2726 - acc: 0.8947 - val_loss: 0.3661 - val_acc: 0.8662\n",
            "Epoch 54/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2480 - acc: 0.9097 - val_loss: 0.3661 - val_acc: 0.8575\n",
            "Epoch 55/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2523 - acc: 0.9129 - val_loss: 0.4051 - val_acc: 0.8575\n",
            "Epoch 56/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2757 - acc: 0.9046 - val_loss: 0.3306 - val_acc: 0.8750\n",
            "Epoch 57/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2601 - acc: 0.9077 - val_loss: 0.3583 - val_acc: 0.8687\n",
            "Epoch 58/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2448 - acc: 0.9100 - val_loss: 0.3467 - val_acc: 0.8612\n",
            "Epoch 59/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2658 - acc: 0.9005 - val_loss: 0.3658 - val_acc: 0.8625\n",
            "Epoch 60/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2437 - acc: 0.9183 - val_loss: 0.3701 - val_acc: 0.8687\n",
            "Epoch 61/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2529 - acc: 0.9085 - val_loss: 0.3532 - val_acc: 0.8662\n",
            "Epoch 62/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2425 - acc: 0.9026 - val_loss: 0.3451 - val_acc: 0.8737\n",
            "Epoch 63/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2374 - acc: 0.9177 - val_loss: 0.3357 - val_acc: 0.8788\n",
            "Epoch 64/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.2334 - acc: 0.9194 - val_loss: 0.3407 - val_acc: 0.8737\n",
            "Epoch 65/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2312 - acc: 0.9264 - val_loss: 0.3355 - val_acc: 0.8800\n",
            "Epoch 66/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2550 - acc: 0.9089 - val_loss: 0.3310 - val_acc: 0.8825\n",
            "Epoch 67/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2434 - acc: 0.9133 - val_loss: 0.3574 - val_acc: 0.8637\n",
            "Epoch 68/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2266 - acc: 0.9208 - val_loss: 0.3889 - val_acc: 0.8612\n",
            "Epoch 69/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2066 - acc: 0.9317 - val_loss: 0.3396 - val_acc: 0.8737\n",
            "Epoch 70/100\n",
            "93/93 [==============================] - 2s 20ms/step - loss: 0.2073 - acc: 0.9308 - val_loss: 0.3981 - val_acc: 0.8662\n",
            "Epoch 71/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1986 - acc: 0.9388 - val_loss: 0.3357 - val_acc: 0.8813\n",
            "Epoch 72/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2231 - acc: 0.9255 - val_loss: 0.3678 - val_acc: 0.8775\n",
            "Epoch 73/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2105 - acc: 0.9326 - val_loss: 0.3475 - val_acc: 0.8838\n",
            "Epoch 74/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1860 - acc: 0.9495 - val_loss: 0.3530 - val_acc: 0.8750\n",
            "Epoch 75/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2212 - acc: 0.9269 - val_loss: 0.4023 - val_acc: 0.8712\n",
            "Epoch 76/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2198 - acc: 0.9292 - val_loss: 0.3610 - val_acc: 0.8800\n",
            "Epoch 77/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2002 - acc: 0.9332 - val_loss: 0.3894 - val_acc: 0.8662\n",
            "Epoch 78/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1864 - acc: 0.9389 - val_loss: 0.3951 - val_acc: 0.8525\n",
            "Epoch 79/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1796 - acc: 0.9432 - val_loss: 0.4108 - val_acc: 0.8712\n",
            "Epoch 80/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.2034 - acc: 0.9255 - val_loss: 0.3609 - val_acc: 0.8863\n",
            "Epoch 81/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1700 - acc: 0.9502 - val_loss: 0.3803 - val_acc: 0.8737\n",
            "Epoch 82/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1514 - acc: 0.9651 - val_loss: 0.3714 - val_acc: 0.8700\n",
            "Epoch 83/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1640 - acc: 0.9600 - val_loss: 0.3804 - val_acc: 0.8775\n",
            "Epoch 84/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1668 - acc: 0.9442 - val_loss: 0.3690 - val_acc: 0.8800\n",
            "Epoch 85/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1540 - acc: 0.9539 - val_loss: 0.3785 - val_acc: 0.8850\n",
            "Epoch 86/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1547 - acc: 0.9623 - val_loss: 0.3709 - val_acc: 0.8863\n",
            "Epoch 87/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1571 - acc: 0.9602 - val_loss: 0.3841 - val_acc: 0.8737\n",
            "Epoch 88/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1559 - acc: 0.9621 - val_loss: 0.4256 - val_acc: 0.8800\n",
            "Epoch 89/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1657 - acc: 0.9476 - val_loss: 0.3715 - val_acc: 0.8838\n",
            "Epoch 90/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1471 - acc: 0.9667 - val_loss: 0.4085 - val_acc: 0.8788\n",
            "Epoch 91/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1599 - acc: 0.9579 - val_loss: 0.3927 - val_acc: 0.8825\n",
            "Epoch 92/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1451 - acc: 0.9595 - val_loss: 0.3958 - val_acc: 0.8825\n",
            "Epoch 93/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1524 - acc: 0.9593 - val_loss: 0.5118 - val_acc: 0.8687\n",
            "Epoch 94/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1671 - acc: 0.9575 - val_loss: 0.4923 - val_acc: 0.8737\n",
            "Epoch 95/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1428 - acc: 0.9563 - val_loss: 0.3958 - val_acc: 0.8913\n",
            "Epoch 96/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1324 - acc: 0.9742 - val_loss: 0.6127 - val_acc: 0.8050\n",
            "Epoch 97/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1361 - acc: 0.9708 - val_loss: 0.4570 - val_acc: 0.8813\n",
            "Epoch 98/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1114 - acc: 0.9734 - val_loss: 0.4358 - val_acc: 0.8800\n",
            "Epoch 99/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1235 - acc: 0.9748 - val_loss: 0.4788 - val_acc: 0.8675\n",
            "Epoch 100/100\n",
            "93/93 [==============================] - 2s 21ms/step - loss: 0.1272 - acc: 0.9703 - val_loss: 0.4687 - val_acc: 0.8788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rwzwr-fM3_Y"
      },
      "source": [
        "### Visualizing The Data\r\n",
        "\r\n",
        "One way to do this is by looking at the distribution of some of the datasetâs variables and make scatter plots to see possible correlations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "nI8FjTjY4KIe",
        "outputId": "546f2277-7a15-456c-b639-7ee47ff65e11"
      },
      "source": [
        "plt = plot(history)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH89us9R_o_F",
        "outputId": "414b29c1-262b-45b4-e94c-95ba29a25905"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_datagen.flow(test_images,\r\n",
        "                                                       test_labels,\r\n",
        "                                                       batch_size=BATCH_SIZE,\r\n",
        "                                                       shuffle=False),\r\n",
        "                                     steps=len(test_images) // BATCH_SIZE,\r\n",
        "                                     callbacks=[GarbageCollectorCallback()])\r\n",
        "\r\n",
        "print(\"Accuracy:\", \"%0.2f\" % (test_acc*100), \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6427 - acc: 0.8406\n",
            "Accuracy: 84.06 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2v_-h75Z3aZ"
      },
      "source": [
        "### Fighting overfitting: 1. Data Augmentation\r\n",
        "\r\n",
        "Neural networks prove not to be very robust to noise, and these plots are characteristic of **overfitting**. Training accuracy keeps increasing linearly while validation accuracy stalls around **82%**. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojvyPceIaunR",
        "outputId": "7e27ab4a-f2c6-4afc-fe86-cf0648c537a2"
      },
      "source": [
        "# Train data augmentation \r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1./65535,\r\n",
        "    rotation_range=40,\r\n",
        "    width_shift_range=0.2,\r\n",
        "    height_shift_range=0.2,\r\n",
        "    shear_range=20,\r\n",
        "    zoom_range=0.2,\r\n",
        "    horizontal_flip=True,\r\n",
        "    fill_mode='nearest')\r\n",
        "\r\n",
        "valid_datagen = ImageDataGenerator(rescale=1./65535)\r\n",
        "test_datagen = ImageDataGenerator(rescale=1./65535) \r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ex5WyyOwir4",
        "outputId": "f9db3001-188f-4bd8-fe19-0c689873d8f5"
      },
      "source": [
        "# Building the new model with the train data augmentation\r\n",
        "model = build_model(\"binary_crossentropy\", \"acc\")\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 3,452,545\n",
            "Trainable params: 3,452,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_LVsSUEj0Vx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc20084-160e-4941-c804-03ff12637ce7"
      },
      "source": [
        "history = model.fit(train_datagen.flow(train_images_split,\r\n",
        "                                       train_labels_split,\r\n",
        "                                       batch_size=BATCH_SIZE,\r\n",
        "                                       shuffle=False),\r\n",
        "                    steps_per_epoch=len(train_images_split) // BATCH_SIZE, \r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    validation_data=valid_datagen.flow(valid_images_split,\r\n",
        "                                       valid_labels_split,\r\n",
        "                                       batch_size=BATCH_SIZE,\r\n",
        "                                       shuffle=False),\r\n",
        "                    validation_steps=len(valid_labels_split) // BATCH_SIZE,\r\n",
        "                    callbacks=[GarbageCollectorCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "93/93 [==============================] - 5s 49ms/step - loss: 1.4353 - acc: 0.5186 - val_loss: 1.0401 - val_acc: 0.5512\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.9636 - acc: 0.5573 - val_loss: 0.7883 - val_acc: 0.5512\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.7651 - acc: 0.5813 - val_loss: 0.6705 - val_acc: 0.6725\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.6835 - acc: 0.6883 - val_loss: 0.6764 - val_acc: 0.5800\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.6413 - acc: 0.7000 - val_loss: 0.5586 - val_acc: 0.7975\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - 4s 45ms/step - loss: 0.6036 - acc: 0.7321 - val_loss: 0.4957 - val_acc: 0.7987\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5786 - acc: 0.7559 - val_loss: 0.4881 - val_acc: 0.8138\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.6103 - acc: 0.7188 - val_loss: 0.4867 - val_acc: 0.8175\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.5793 - acc: 0.7486 - val_loss: 0.4831 - val_acc: 0.8263\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5628 - acc: 0.7486 - val_loss: 0.5104 - val_acc: 0.7962\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5540 - acc: 0.7718 - val_loss: 0.5183 - val_acc: 0.7825\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5564 - acc: 0.7490 - val_loss: 0.4995 - val_acc: 0.7950\n",
            "Epoch 13/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5693 - acc: 0.7462 - val_loss: 0.5164 - val_acc: 0.7850\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5577 - acc: 0.7521 - val_loss: 0.4485 - val_acc: 0.8300\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5609 - acc: 0.7568 - val_loss: 0.4827 - val_acc: 0.8125\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5670 - acc: 0.7566 - val_loss: 0.4351 - val_acc: 0.8338\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5532 - acc: 0.7694 - val_loss: 0.5739 - val_acc: 0.7450\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5371 - acc: 0.7528 - val_loss: 0.7266 - val_acc: 0.6025\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5813 - acc: 0.7427 - val_loss: 0.5540 - val_acc: 0.7350\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5252 - acc: 0.7546 - val_loss: 0.4371 - val_acc: 0.8363\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5403 - acc: 0.7501 - val_loss: 0.4297 - val_acc: 0.8163\n",
            "Epoch 22/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5330 - acc: 0.7642 - val_loss: 0.4431 - val_acc: 0.8175\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - 4s 45ms/step - loss: 0.5098 - acc: 0.7865 - val_loss: 0.4564 - val_acc: 0.8012\n",
            "Epoch 24/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5139 - acc: 0.7759 - val_loss: 0.4982 - val_acc: 0.7663\n",
            "Epoch 25/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5340 - acc: 0.7669 - val_loss: 0.4920 - val_acc: 0.7750\n",
            "Epoch 26/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5111 - acc: 0.7925 - val_loss: 0.4022 - val_acc: 0.8587\n",
            "Epoch 27/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4925 - acc: 0.7886 - val_loss: 0.4855 - val_acc: 0.7588\n",
            "Epoch 28/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5022 - acc: 0.7916 - val_loss: 0.5369 - val_acc: 0.6875\n",
            "Epoch 29/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.5081 - acc: 0.7723 - val_loss: 0.4158 - val_acc: 0.8338\n",
            "Epoch 30/100\n",
            "93/93 [==============================] - 4s 45ms/step - loss: 0.4881 - acc: 0.7972 - val_loss: 0.4100 - val_acc: 0.8350\n",
            "Epoch 31/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4967 - acc: 0.7802 - val_loss: 0.3984 - val_acc: 0.8687\n",
            "Epoch 32/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4910 - acc: 0.7951 - val_loss: 0.4181 - val_acc: 0.8662\n",
            "Epoch 33/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4831 - acc: 0.7938 - val_loss: 0.4835 - val_acc: 0.7800\n",
            "Epoch 34/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4848 - acc: 0.7897 - val_loss: 0.3673 - val_acc: 0.8612\n",
            "Epoch 35/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4746 - acc: 0.7988 - val_loss: 0.4958 - val_acc: 0.8050\n",
            "Epoch 36/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.4810 - acc: 0.8016 - val_loss: 0.3561 - val_acc: 0.8550\n",
            "Epoch 37/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4635 - acc: 0.8026 - val_loss: 0.3945 - val_acc: 0.8200\n",
            "Epoch 38/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4780 - acc: 0.8099 - val_loss: 0.3484 - val_acc: 0.8650\n",
            "Epoch 39/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4453 - acc: 0.8247 - val_loss: 0.3782 - val_acc: 0.8338\n",
            "Epoch 40/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4605 - acc: 0.8041 - val_loss: 0.3727 - val_acc: 0.8612\n",
            "Epoch 41/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4357 - acc: 0.8303 - val_loss: 0.3892 - val_acc: 0.8213\n",
            "Epoch 42/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4815 - acc: 0.8129 - val_loss: 0.4599 - val_acc: 0.7825\n",
            "Epoch 43/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4553 - acc: 0.8204 - val_loss: 0.4505 - val_acc: 0.7962\n",
            "Epoch 44/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4583 - acc: 0.8119 - val_loss: 0.3758 - val_acc: 0.8725\n",
            "Epoch 45/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4331 - acc: 0.8315 - val_loss: 0.3516 - val_acc: 0.8675\n",
            "Epoch 46/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4491 - acc: 0.8201 - val_loss: 0.3617 - val_acc: 0.8575\n",
            "Epoch 47/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4496 - acc: 0.8261 - val_loss: 0.3985 - val_acc: 0.8100\n",
            "Epoch 48/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4484 - acc: 0.8400 - val_loss: 0.3984 - val_acc: 0.8213\n",
            "Epoch 49/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4566 - acc: 0.8171 - val_loss: 0.3644 - val_acc: 0.8725\n",
            "Epoch 50/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4249 - acc: 0.8283 - val_loss: 0.3315 - val_acc: 0.8900\n",
            "Epoch 51/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4337 - acc: 0.8263 - val_loss: 0.4079 - val_acc: 0.8025\n",
            "Epoch 52/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4370 - acc: 0.8276 - val_loss: 0.3755 - val_acc: 0.8825\n",
            "Epoch 53/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4435 - acc: 0.8115 - val_loss: 0.3424 - val_acc: 0.8788\n",
            "Epoch 54/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4009 - acc: 0.8386 - val_loss: 0.3352 - val_acc: 0.8850\n",
            "Epoch 55/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4294 - acc: 0.8211 - val_loss: 0.3161 - val_acc: 0.9013\n",
            "Epoch 56/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4086 - acc: 0.8435 - val_loss: 0.3865 - val_acc: 0.8263\n",
            "Epoch 57/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4176 - acc: 0.8309 - val_loss: 0.3553 - val_acc: 0.8525\n",
            "Epoch 58/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4120 - acc: 0.8380 - val_loss: 0.3463 - val_acc: 0.8900\n",
            "Epoch 59/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.4368 - acc: 0.8280 - val_loss: 0.3963 - val_acc: 0.8800\n",
            "Epoch 60/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4372 - acc: 0.8335 - val_loss: 0.3720 - val_acc: 0.8438\n",
            "Epoch 61/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4072 - acc: 0.8284 - val_loss: 0.3128 - val_acc: 0.8975\n",
            "Epoch 62/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4028 - acc: 0.8396 - val_loss: 0.3227 - val_acc: 0.8662\n",
            "Epoch 63/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4089 - acc: 0.8343 - val_loss: 0.4207 - val_acc: 0.8050\n",
            "Epoch 64/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4180 - acc: 0.8384 - val_loss: 0.3670 - val_acc: 0.8587\n",
            "Epoch 65/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.4005 - acc: 0.8378 - val_loss: 0.3397 - val_acc: 0.8575\n",
            "Epoch 66/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.4264 - acc: 0.8280 - val_loss: 0.4293 - val_acc: 0.8200\n",
            "Epoch 67/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3950 - acc: 0.8447 - val_loss: 0.4438 - val_acc: 0.7862\n",
            "Epoch 68/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4159 - acc: 0.8300 - val_loss: 0.3373 - val_acc: 0.8813\n",
            "Epoch 69/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4215 - acc: 0.8418 - val_loss: 0.3594 - val_acc: 0.8500\n",
            "Epoch 70/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4089 - acc: 0.8274 - val_loss: 0.3585 - val_acc: 0.8375\n",
            "Epoch 71/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.3942 - acc: 0.8563 - val_loss: 0.3295 - val_acc: 0.8650\n",
            "Epoch 72/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.3852 - acc: 0.8623 - val_loss: 0.3442 - val_acc: 0.8562\n",
            "Epoch 73/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.4010 - acc: 0.8478 - val_loss: 0.3242 - val_acc: 0.8700\n",
            "Epoch 74/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3886 - acc: 0.8489 - val_loss: 0.3800 - val_acc: 0.8650\n",
            "Epoch 75/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3867 - acc: 0.8407 - val_loss: 0.4266 - val_acc: 0.8037\n",
            "Epoch 76/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3906 - acc: 0.8560 - val_loss: 0.4905 - val_acc: 0.7513\n",
            "Epoch 77/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4068 - acc: 0.8342 - val_loss: 0.3395 - val_acc: 0.8675\n",
            "Epoch 78/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.4121 - acc: 0.8453 - val_loss: 0.3072 - val_acc: 0.8875\n",
            "Epoch 79/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.3660 - acc: 0.8456 - val_loss: 0.3212 - val_acc: 0.8825\n",
            "Epoch 80/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.4200 - acc: 0.8238 - val_loss: 0.3885 - val_acc: 0.8388\n",
            "Epoch 81/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3709 - acc: 0.8534 - val_loss: 0.3247 - val_acc: 0.8725\n",
            "Epoch 82/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3646 - acc: 0.8628 - val_loss: 0.3189 - val_acc: 0.8825\n",
            "Epoch 83/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4105 - acc: 0.8446 - val_loss: 0.3362 - val_acc: 0.8825\n",
            "Epoch 84/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3794 - acc: 0.8416 - val_loss: 0.3313 - val_acc: 0.8888\n",
            "Epoch 85/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.4054 - acc: 0.8474 - val_loss: 0.3062 - val_acc: 0.8750\n",
            "Epoch 86/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3673 - acc: 0.8686 - val_loss: 0.3247 - val_acc: 0.8750\n",
            "Epoch 87/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3639 - acc: 0.8443 - val_loss: 0.3655 - val_acc: 0.8512\n",
            "Epoch 88/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3727 - acc: 0.8578 - val_loss: 0.3412 - val_acc: 0.8913\n",
            "Epoch 89/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3896 - acc: 0.8410 - val_loss: 0.3458 - val_acc: 0.8562\n",
            "Epoch 90/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3836 - acc: 0.8510 - val_loss: 0.3112 - val_acc: 0.8875\n",
            "Epoch 91/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.3722 - acc: 0.8597 - val_loss: 0.3394 - val_acc: 0.8725\n",
            "Epoch 92/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.4061 - acc: 0.8400 - val_loss: 0.3303 - val_acc: 0.8900\n",
            "Epoch 93/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3433 - acc: 0.8683 - val_loss: 0.3391 - val_acc: 0.8575\n",
            "Epoch 94/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3791 - acc: 0.8447 - val_loss: 0.3636 - val_acc: 0.8537\n",
            "Epoch 95/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3724 - acc: 0.8463 - val_loss: 0.3010 - val_acc: 0.8825\n",
            "Epoch 96/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.3826 - acc: 0.8390 - val_loss: 0.3248 - val_acc: 0.8838\n",
            "Epoch 97/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3386 - acc: 0.8728 - val_loss: 0.3158 - val_acc: 0.8850\n",
            "Epoch 98/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.3537 - acc: 0.8659 - val_loss: 0.2928 - val_acc: 0.8900\n",
            "Epoch 99/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.3626 - acc: 0.8550 - val_loss: 0.3037 - val_acc: 0.8863\n",
            "Epoch 100/100\n",
            "93/93 [==============================] - 4s 47ms/step - loss: 0.3587 - acc: 0.8553 - val_loss: 0.3130 - val_acc: 0.8900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "DNJBV5khoFEA",
        "outputId": "69dbec15-7c86-407e-b09f-21a36dafd453"
      },
      "source": [
        "plt = plot(history)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYDj_8GquRRh",
        "outputId": "264c8125-d6da-4b6d-d97c-4f105e707092"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_datagen.flow(test_images,\r\n",
        "                                                       test_labels,\r\n",
        "                                                       batch_size=BATCH_SIZE,\r\n",
        "                                                       shuffle=False),\r\n",
        "                                     steps=len(test_images) // BATCH_SIZE,\r\n",
        "                                     callbacks=[GarbageCollectorCallback()])\r\n",
        "\r\n",
        "print(\"Accuracy:\", \"%0.2f\" % (test_acc*100), \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3553 - acc: 0.8438\n",
            "Accuracy: 84.38 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrLD4fBGtc6k"
      },
      "source": [
        "##K-fold cross validation.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuiUODCT97Bq",
        "outputId": "fe0f3ded-4c3b-4ac6-eb89-b329a75d2f80"
      },
      "source": [
        "# Train data augmentation for k-fold-cross-validation \r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1./65535,\r\n",
        "    rotation_range=40,\r\n",
        "    width_shift_range=0.2,\r\n",
        "    height_shift_range=0.2,\r\n",
        "    shear_range=20,\r\n",
        "    zoom_range=0.2,\r\n",
        "    horizontal_flip=True,\r\n",
        "    fill_mode='nearest')\r\n",
        "\r\n",
        "valid_datagen = ImageDataGenerator(rescale=1./65535)\r\n",
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2676, 150, 150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgdAZtZ1wMf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "debab907-da4e-4ff6-99ba-93a203b1ec6f"
      },
      "source": [
        "def cross_validate(k, batch_size, num_epochs, dataset, targets, verbosity):\r\n",
        "  #10-Fold-Cross-Validation\r\n",
        "  num_val_samples = len(dataset) // k \r\n",
        "  validation_accuracies = []\r\n",
        "  validation_losses = []\r\n",
        "\r\n",
        "  for i in range(k):\r\n",
        "    # rigen augmented data \r\n",
        "    \r\n",
        "    print(\"processing fold #\", i)\r\n",
        "    validation_data = dataset[i * num_val_samples : (i + 1) * num_val_samples]\r\n",
        "    validation_labels = targets[i * num_val_samples : (i + 1) * num_val_samples]\r\n",
        "\r\n",
        "    partial_train_data = np.concatenate(\r\n",
        "        [dataset[:i * num_val_samples],\r\n",
        "        dataset[(i + 1) * num_val_samples:]], \r\n",
        "        axis=0)\r\n",
        "\r\n",
        "    partial_train_targets = np.concatenate(\r\n",
        "        [targets[:i * num_val_samples],\r\n",
        "        targets[(i + 1) * num_val_samples:]], \r\n",
        "        axis=0)\r\n",
        "\r\n",
        "    model = build_model(\"binary_crossentropy\", \"acc\")\r\n",
        "    \r\n",
        "    history = model.fit(train_datagen.flow(partial_train_data, \r\n",
        "                                          partial_train_targets,\r\n",
        "                                          batch_size=batch_size,\r\n",
        "                                          shuffle=False),\r\n",
        "                        epochs=num_epochs,\r\n",
        "                        steps_per_epoch=len(partial_train_data) // batch_size,\r\n",
        "                        verbose=verbosity,\r\n",
        "                        callbacks=[GarbageCollectorCallback()])\r\n",
        "    \r\n",
        "    val_loss, val_acc = model.evaluate(valid_datagen.flow(validation_data,\r\n",
        "                                                          validation_labels,\r\n",
        "                                                          batch_size=batch_size,\r\n",
        "                                                          shuffle=False),\r\n",
        "                                       steps=len(validation_data) // batch_size,\r\n",
        "                                       callbacks=[GarbageCollectorCallback()])\r\n",
        "    \r\n",
        "    validation_accuracies.append(val_acc)\r\n",
        "    validation_losses.append(val_loss)\r\n",
        "\r\n",
        "  return validation_accuracies, validation_losses \r\n",
        "  \r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdmZ-0dKuKlG"
      },
      "source": [
        "acc, loss = cross_validate(k=10, batch_size=20, num_epochs=100, dataset=train_images, targets=train_labels, verbosity=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3hbM3c6RxTq"
      },
      "source": [
        "print(len(acc))\r\n",
        "print(len(loss))\r\n",
        "print()\r\n",
        "print(np.mean(acc))\r\n",
        "print(np.mean(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQKeyLVTVi7"
      },
      "source": [
        "### Hyperparameters Tuning\r\n",
        "\r\n",
        "To compare the performance of one machine learning algorithm to another, it is necessary to perform controlled experiments\r\n",
        "\r\n",
        "*   add Dropout or L2 Regularization\r\n",
        "*   varying of convolutional layers: [3, 5, 7]\r\n",
        "*   varying # of units per layer:\r\n",
        "\r\n",
        "| Layer  | Unit per Layer  |  \r\n",
        "|---|---|\r\n",
        "| 1  | [32,32,64,128]  |\r\n",
        "| 2  | [32,64,128,128]  |\r\n",
        "| 3  | [32,64,128,256]  |\r\n",
        "| 4  | [64,64,128,256]  |\r\n",
        "\r\n",
        "*   change Optimizer (try Adam)\r\n",
        "*   varying batch size: [20, 32, 64, 128]\r\n",
        "*   varying learning rate \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO1GEdyfkbRO"
      },
      "source": [
        "#Parameters Grid \r\n",
        "dropout_regularization = True                                               # grafico comparativo: sembra leggermente meglio con Dropout\r\n",
        "batch_sizes = [20, 32, 64, 128]                                             # 4 \r\n",
        "layers_number = 4                                                           # 1\r\n",
        "units_per_layer_dict = [[32,64,128,128], [32,64,128,256]]                   # 2\r\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]                                         # 3\r\n",
        "num_epochs = 100\r\n",
        "num_folds = 5\r\n",
        "\r\n",
        "file_path_out = os.path.join(base_path, \"tuning results/top3_results.csv\")\r\n",
        "file_path = os.path.join(base_path, \"tuning results/results_full.csv\")\r\n",
        "model_path = os.path.join(base_path, \"tuning results/best_model.h5\")\r\n",
        "\r\n",
        "# Top k models \r\n",
        "k = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQgC-nrCTcn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e473ecba-8e90-45c5-ed39-e9014586f0ed"
      },
      "source": [
        "def build_custom_model(layers_number, units_per_layer, batch_size, dropout, optimizer):\r\n",
        "  model = models.Sequential()\r\n",
        "\r\n",
        "  for i in range(layers_number):\r\n",
        "     # First layer. Setting input shape\r\n",
        "    if i == 0: \r\n",
        "      model.add(layers.Conv2D(units_per_layer[i], (3, 3), activation='relu', input_shape=(150, 150, 1)))\r\n",
        "      model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "    else:\r\n",
        "      model.add(layers.Conv2D(units_per_layer[i], (3, 3), activation='relu'))\r\n",
        "      model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "\r\n",
        "  model.add(layers.Flatten())\r\n",
        "  model.add(layers.Dense(512, activation='relu'))\r\n",
        "  \r\n",
        "  if dropout:\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "\r\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "  model.compile(loss=\"binary_crossentropy\",\r\n",
        "              optimizer=optimizer,\r\n",
        "              metrics=[\"acc\"]) \r\n",
        "  \r\n",
        "  return model \r\n",
        "\r\n",
        "def CNN_tuning(num_folds, batch_sizes, num_epochs, layers_number, units_per_layer_dict, learning_rates, dropout_reg, dataset, targets):\r\n",
        "  lr = learning_rates[0]\r\n",
        "  opts = [optimizers.RMSprop(lr=lr), optimizers.Adam(learning_rate=lr)]\r\n",
        "\r\n",
        "  with open(file_path, 'w') as f:\r\n",
        "    header = \"batch_size,num_epochs,units_per_layer,optimizer,learning_rate,mean_val_acc,mean_val_loss,num_folds\\n\"\r\n",
        "    f.write(header)\r\n",
        "\r\n",
        "  for batch_size in batch_sizes:                # 4\r\n",
        "    for opt in opts:                            # 2\r\n",
        "      for values_set in units_per_layer_dict:   # 2\r\n",
        "        # Build CNN model \r\n",
        "        model = build_custom_model(layers_number, values_set, batch_size, dropout_reg, opt)\r\n",
        "\r\n",
        "        if \"RMSprop\" in str(opt):\r\n",
        "          str_opt = \"RMSprop\"  \r\n",
        "        else:\r\n",
        "          str_opt = \"Adam\"\r\n",
        "\r\n",
        "        #print info \r\n",
        "        print(\"-----------------------------------------------------\")\r\n",
        "        print(\"batch_size: \\t\", batch_size)\r\n",
        "        print(\"num_epochs: \\t\", num_epochs)\r\n",
        "        print(\"units_per_layer:\", str(values_set).replace(\",\", \" \"))\r\n",
        "        print(\"optimizer: \\t\", str_opt)\r\n",
        "        print(\"learning_rate: \\t\", str(lr))\r\n",
        "        print(\"num_folds CV: \\t\", num_folds)\r\n",
        "        print(\"-----------------------------------------------------\")\r\n",
        "\r\n",
        "        #cross validate CNN model\r\n",
        "        val_acc, val_loss = cross_validate(num_folds, batch_size, num_epochs, dataset, targets, 1)\r\n",
        "\r\n",
        "        #save results on csv file \r\n",
        "        with open(file_path, 'a') as f:\r\n",
        "          row = str(batch_size) + \",\" \\\r\n",
        "              + str(num_epochs) + \",\" \\\r\n",
        "              + str(values_set).replace(\",\", \" \") + \",\" \\\r\n",
        "              + str_opt + \",\" \\\r\n",
        "              + str(lr) + \",\" \\\r\n",
        "              + \"%0.4f (+/- %0.4f)\" % (np.mean(val_acc), np.std(val_acc) * 2) + \",\" \\\r\n",
        "              + \"%0.4f (+/- %0.4f)\" % (np.mean(val_loss), np.std(val_loss) * 2) + \",\" \\\r\n",
        "              + str(num_folds) + \"\\n\"\r\n",
        "          f.write(row)\r\n",
        "\r\n",
        "        del model\r\n",
        "        K.clear_session()\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0fLlSfbPOWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01add45f-8daa-4a89-80e3-fd084df2a0d8"
      },
      "source": [
        "CNN_tuning(num_folds, batch_sizes, num_epochs, layers_number, units_per_layer_dict, learning_rates, dropout_regularization, train_images, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mSe han truncado las Ãºltimas 5000 lÃ­neas del flujo de salida.\u001b[0m\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4123 - acc: 0.8387\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4143 - acc: 0.8260\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4336 - acc: 0.8214\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3993 - acc: 0.8322\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4096 - acc: 0.8206\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4006 - acc: 0.8524\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3946 - acc: 0.8490\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4017 - acc: 0.8247\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3876 - acc: 0.8328\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3965 - acc: 0.8426\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4303 - acc: 0.8081\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3941 - acc: 0.8435\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3596 - acc: 0.8625\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3758 - acc: 0.8610\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3919 - acc: 0.8464\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3885 - acc: 0.8344\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3569 - acc: 0.8495\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3882 - acc: 0.8512\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3938 - acc: 0.8475\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3862 - acc: 0.8385\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3757 - acc: 0.8474\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3666 - acc: 0.8648\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3781 - acc: 0.8519\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3803 - acc: 0.8438\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3804 - acc: 0.8479\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3651 - acc: 0.8567\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3796 - acc: 0.8350\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3519 - acc: 0.8653\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3728 - acc: 0.8514\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3657 - acc: 0.8484\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3428 - acc: 0.8540\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3874 - acc: 0.8423\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3719 - acc: 0.8428\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3701 - acc: 0.8599\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3589 - acc: 0.8671\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3550 - acc: 0.8567\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3674 - acc: 0.8588\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3510 - acc: 0.8606\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3376 - acc: 0.8622\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3479 - acc: 0.8657\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3210 - acc: 0.8734\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3604 - acc: 0.8518\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3544 - acc: 0.8556\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3429 - acc: 0.8733\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3649 - acc: 0.8557\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3784 - acc: 0.8441\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2648 - acc: 0.8981\n",
            "processing fold # 1\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 1.4014 - acc: 0.5334\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.9083 - acc: 0.5384\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.7373 - acc: 0.5780\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.7081 - acc: 0.5584\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.6731 - acc: 0.6442\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.6529 - acc: 0.6624\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5990 - acc: 0.7138\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5695 - acc: 0.7384\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5698 - acc: 0.7397\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5638 - acc: 0.7314\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5690 - acc: 0.7431\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5745 - acc: 0.7316\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5387 - acc: 0.7541\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5475 - acc: 0.7443\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5386 - acc: 0.7429\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5376 - acc: 0.7474\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5382 - acc: 0.7519\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5513 - acc: 0.7443\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5346 - acc: 0.7515\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5109 - acc: 0.7700\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5219 - acc: 0.7605\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5311 - acc: 0.7501\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5358 - acc: 0.7523\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4961 - acc: 0.7675\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5209 - acc: 0.7626\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4969 - acc: 0.7802\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5154 - acc: 0.7754\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5040 - acc: 0.7790\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.4705 - acc: 0.7877\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4808 - acc: 0.7839\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4659 - acc: 0.7951\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4992 - acc: 0.7777\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4878 - acc: 0.7791\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5024 - acc: 0.7721\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4817 - acc: 0.7945\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4509 - acc: 0.7920\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4502 - acc: 0.8168\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4606 - acc: 0.8073\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4666 - acc: 0.7990\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4531 - acc: 0.8114\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4559 - acc: 0.7973\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4719 - acc: 0.7964\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4514 - acc: 0.8110\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4478 - acc: 0.8166\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4396 - acc: 0.8112\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4339 - acc: 0.8339\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4336 - acc: 0.8215\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4422 - acc: 0.8166\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4507 - acc: 0.8088\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4299 - acc: 0.8341\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4508 - acc: 0.8156\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4328 - acc: 0.8351\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4084 - acc: 0.8412\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4145 - acc: 0.8311\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4231 - acc: 0.8270\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4386 - acc: 0.8237\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4304 - acc: 0.8320\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4138 - acc: 0.8264\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4206 - acc: 0.8260\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4254 - acc: 0.8211\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4105 - acc: 0.8388\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4219 - acc: 0.8485\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4169 - acc: 0.8281\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4091 - acc: 0.8339\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3985 - acc: 0.8278\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4111 - acc: 0.8381\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4018 - acc: 0.8470\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4011 - acc: 0.8467\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3828 - acc: 0.8455\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4055 - acc: 0.8259\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3736 - acc: 0.8516\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3878 - acc: 0.8415\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3992 - acc: 0.8342\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3820 - acc: 0.8588\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3873 - acc: 0.8512\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3960 - acc: 0.8420\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4123 - acc: 0.8327\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3881 - acc: 0.8386\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3774 - acc: 0.8461\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3742 - acc: 0.8511\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3818 - acc: 0.8492\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4131 - acc: 0.8222\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3799 - acc: 0.8457\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3696 - acc: 0.8546\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3935 - acc: 0.8469\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3844 - acc: 0.8497\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3812 - acc: 0.8519\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3498 - acc: 0.8730\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3781 - acc: 0.8455\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3528 - acc: 0.8625\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3859 - acc: 0.8522\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3806 - acc: 0.8447\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3715 - acc: 0.8492\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3871 - acc: 0.8357\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3636 - acc: 0.8503\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3835 - acc: 0.8470\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3805 - acc: 0.8532\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3710 - acc: 0.8429\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3679 - acc: 0.8556\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3657 - acc: 0.8529\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3545 - acc: 0.8500\n",
            "processing fold # 2\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 1.4068 - acc: 0.5324\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.9108 - acc: 0.5567\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.7354 - acc: 0.5985\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6844 - acc: 0.6709\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.6387 - acc: 0.7198\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.5927 - acc: 0.7398\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5987 - acc: 0.7389\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.6154 - acc: 0.7161\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.5679 - acc: 0.7500\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.5629 - acc: 0.7498\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.5820 - acc: 0.7418\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5820 - acc: 0.7259\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5569 - acc: 0.7620\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5364 - acc: 0.7642\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5507 - acc: 0.7527\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4986 - acc: 0.7906\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5473 - acc: 0.7547\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5337 - acc: 0.7518\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5188 - acc: 0.7922\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5183 - acc: 0.7897\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5220 - acc: 0.7759\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5056 - acc: 0.7813\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5106 - acc: 0.7824\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4771 - acc: 0.7947\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5100 - acc: 0.7783\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4772 - acc: 0.8059\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4777 - acc: 0.7971\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4891 - acc: 0.8014\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4727 - acc: 0.8035\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4686 - acc: 0.7907\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4972 - acc: 0.7888\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4469 - acc: 0.8073\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4987 - acc: 0.7756\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4627 - acc: 0.8009\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4899 - acc: 0.7761\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4805 - acc: 0.8000\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4617 - acc: 0.7941\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4796 - acc: 0.7927\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4561 - acc: 0.8163\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4644 - acc: 0.7950\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4453 - acc: 0.8142\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4393 - acc: 0.8179\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4423 - acc: 0.8269\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4354 - acc: 0.8285\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4486 - acc: 0.8179\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4461 - acc: 0.8098\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4496 - acc: 0.8271\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4371 - acc: 0.8396\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4363 - acc: 0.8206\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4330 - acc: 0.8326\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4399 - acc: 0.8268\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4248 - acc: 0.8196\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4431 - acc: 0.8283\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4352 - acc: 0.8245\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4440 - acc: 0.8167\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4127 - acc: 0.8329\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4238 - acc: 0.8312\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4406 - acc: 0.8211\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4229 - acc: 0.8375\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4321 - acc: 0.8202\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4322 - acc: 0.8421\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4334 - acc: 0.8278\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4031 - acc: 0.8281\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4261 - acc: 0.8268\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4206 - acc: 0.8429\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4112 - acc: 0.8432\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4048 - acc: 0.8423\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3929 - acc: 0.8464\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3990 - acc: 0.8509\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3844 - acc: 0.8347\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3888 - acc: 0.8408\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3731 - acc: 0.8517\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3919 - acc: 0.8409\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4055 - acc: 0.8303\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3984 - acc: 0.8500\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3912 - acc: 0.8467\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4009 - acc: 0.8448\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4031 - acc: 0.8448\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4042 - acc: 0.8390\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3806 - acc: 0.8512\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4190 - acc: 0.8344\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3958 - acc: 0.8389\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3789 - acc: 0.8438\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3931 - acc: 0.8478\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4075 - acc: 0.8304\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.3894 - acc: 0.8351\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3556 - acc: 0.8637\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3758 - acc: 0.8523\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3661 - acc: 0.8645\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3814 - acc: 0.8495\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3819 - acc: 0.8483\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3657 - acc: 0.8566\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3818 - acc: 0.8464\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3717 - acc: 0.8464\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3895 - acc: 0.8468\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3846 - acc: 0.8538\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3663 - acc: 0.8502\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3574 - acc: 0.8621\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3672 - acc: 0.8584\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3836 - acc: 0.8357\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.4035 - acc: 0.8385\n",
            "processing fold # 3\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 39ms/step - loss: 1.4166 - acc: 0.5409\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.9302 - acc: 0.5409\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.7477 - acc: 0.5948\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6951 - acc: 0.6300\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6451 - acc: 0.7130\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6291 - acc: 0.7082\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6088 - acc: 0.7244\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6003 - acc: 0.7110\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5943 - acc: 0.7272\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5761 - acc: 0.7328\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5442 - acc: 0.7595\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5578 - acc: 0.7439\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5598 - acc: 0.7357\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5362 - acc: 0.7570\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5334 - acc: 0.7635\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5371 - acc: 0.7689\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5299 - acc: 0.7779\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5134 - acc: 0.7786\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5218 - acc: 0.7580\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5200 - acc: 0.7642\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5450 - acc: 0.7652\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5452 - acc: 0.7507\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5067 - acc: 0.7857\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5229 - acc: 0.7769\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4749 - acc: 0.7990\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5065 - acc: 0.7907\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4904 - acc: 0.8051\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4916 - acc: 0.7855\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4622 - acc: 0.8072\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4650 - acc: 0.8067\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.5168 - acc: 0.7785\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4645 - acc: 0.8054\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4666 - acc: 0.8024\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4646 - acc: 0.8156\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4665 - acc: 0.8092\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4330 - acc: 0.8224\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.4806 - acc: 0.7942\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4631 - acc: 0.8088\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4559 - acc: 0.8035\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4462 - acc: 0.8117\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4438 - acc: 0.8222\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4451 - acc: 0.8328\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.4448 - acc: 0.8271\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4435 - acc: 0.8197\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4431 - acc: 0.8234\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4434 - acc: 0.8206\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4573 - acc: 0.8091\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4500 - acc: 0.8225\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4591 - acc: 0.8083\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.4608 - acc: 0.8123\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4241 - acc: 0.8237\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4221 - acc: 0.8282\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4137 - acc: 0.8385\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3954 - acc: 0.8472\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4500 - acc: 0.8207\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3994 - acc: 0.8315\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4194 - acc: 0.8430\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4295 - acc: 0.8259\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4346 - acc: 0.8246\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4034 - acc: 0.8421\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4167 - acc: 0.8358\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4192 - acc: 0.8266\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4084 - acc: 0.8348\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.4126 - acc: 0.8267\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.4000 - acc: 0.8427\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4004 - acc: 0.8312\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3987 - acc: 0.8488\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4084 - acc: 0.8370\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.3977 - acc: 0.8408\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4052 - acc: 0.8384\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3854 - acc: 0.8533\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3871 - acc: 0.8530\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3901 - acc: 0.8353\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3978 - acc: 0.8442\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.3867 - acc: 0.8426\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3987 - acc: 0.8274\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3902 - acc: 0.8433\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4029 - acc: 0.8465\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4106 - acc: 0.8322\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4118 - acc: 0.8363\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3807 - acc: 0.8497\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3721 - acc: 0.8505\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3933 - acc: 0.8424\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3727 - acc: 0.8604\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3779 - acc: 0.8458\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3710 - acc: 0.8476\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3472 - acc: 0.8640\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3748 - acc: 0.8535\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3709 - acc: 0.8602\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3617 - acc: 0.8585\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3810 - acc: 0.8555\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3839 - acc: 0.8530\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3578 - acc: 0.8550\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3851 - acc: 0.8340\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3718 - acc: 0.8560\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3627 - acc: 0.8510\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3678 - acc: 0.8516\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3760 - acc: 0.8525\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3584 - acc: 0.8668\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.3585 - acc: 0.8604\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2490 - acc: 0.9038\n",
            "processing fold # 4\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 39ms/step - loss: 1.4124 - acc: 0.5243\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.9168 - acc: 0.5422\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.7472 - acc: 0.5797\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6930 - acc: 0.6525\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6380 - acc: 0.6991\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6092 - acc: 0.7335\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.6009 - acc: 0.7283\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5734 - acc: 0.7462\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5878 - acc: 0.7358\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5775 - acc: 0.7327\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5423 - acc: 0.7586\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5635 - acc: 0.7340\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5633 - acc: 0.7426\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5406 - acc: 0.7683\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5609 - acc: 0.7503\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5606 - acc: 0.7624\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5410 - acc: 0.7705\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5162 - acc: 0.7662\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5212 - acc: 0.7600\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5258 - acc: 0.7630\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5202 - acc: 0.7650\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.4914 - acc: 0.7885\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5464 - acc: 0.7563\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4970 - acc: 0.7811\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5163 - acc: 0.7889\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5028 - acc: 0.7801\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5194 - acc: 0.7800\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4994 - acc: 0.7879\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4830 - acc: 0.7990\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4934 - acc: 0.7888\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5101 - acc: 0.7827\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.4878 - acc: 0.7858\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4519 - acc: 0.8149\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4732 - acc: 0.8007\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4700 - acc: 0.8035\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.4638 - acc: 0.8074\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4782 - acc: 0.7993\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4794 - acc: 0.8010\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4527 - acc: 0.8271\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4896 - acc: 0.8004\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4210 - acc: 0.8283\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4488 - acc: 0.8181\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4515 - acc: 0.8233\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4402 - acc: 0.8137\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4434 - acc: 0.8088\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4520 - acc: 0.8195\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4332 - acc: 0.8314\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4426 - acc: 0.8295\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4457 - acc: 0.8147\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4391 - acc: 0.8333\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4580 - acc: 0.8033\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4433 - acc: 0.8046\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4147 - acc: 0.8327\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4458 - acc: 0.8310\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4274 - acc: 0.8249\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4384 - acc: 0.8209\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4510 - acc: 0.8028\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4202 - acc: 0.8222\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4246 - acc: 0.8240\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4218 - acc: 0.8337\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3965 - acc: 0.8439\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4228 - acc: 0.8150\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4059 - acc: 0.8423\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4205 - acc: 0.8354\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4023 - acc: 0.8355\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4119 - acc: 0.8417\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4218 - acc: 0.8263\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4410 - acc: 0.8197\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3897 - acc: 0.8496\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3931 - acc: 0.8483\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4150 - acc: 0.8258\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4102 - acc: 0.8554\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4176 - acc: 0.8315\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3867 - acc: 0.8452\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3945 - acc: 0.8418\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3957 - acc: 0.8382\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3817 - acc: 0.8536\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3602 - acc: 0.8639\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 4s 38ms/step - loss: 0.3659 - acc: 0.8637\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3882 - acc: 0.8412\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3925 - acc: 0.8492\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4040 - acc: 0.8369\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3790 - acc: 0.8469\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3962 - acc: 0.8438\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3707 - acc: 0.8513\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3824 - acc: 0.8358\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3920 - acc: 0.8508\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3596 - acc: 0.8636\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3941 - acc: 0.8329\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3663 - acc: 0.8521\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3712 - acc: 0.8478\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3715 - acc: 0.8514\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3575 - acc: 0.8660\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3552 - acc: 0.8577\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3951 - acc: 0.8468\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3678 - acc: 0.8473\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3758 - acc: 0.8482\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3986 - acc: 0.8438\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3738 - acc: 0.8435\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3630 - acc: 0.8630\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.4168 - acc: 0.8173\n",
            "-----------------------------------------------------\n",
            "batch_size: \t 20\n",
            "num_epochs: \t 100\n",
            "units_per_layer: [32  64  128  256]\n",
            "optimizer: \t RMSprop\n",
            "learning_rate: \t 0.01\n",
            "num_folds CV: \t 5\n",
            "-----------------------------------------------------\n",
            "processing fold # 0\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 40ms/step - loss: 1.4214 - acc: 0.5191\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.9206 - acc: 0.5543\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.7569 - acc: 0.6000\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6765 - acc: 0.6764\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6329 - acc: 0.7102\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6259 - acc: 0.7169\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6259 - acc: 0.7279\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5918 - acc: 0.7356\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5929 - acc: 0.7280\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5940 - acc: 0.7299\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5639 - acc: 0.7470\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5560 - acc: 0.7566\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5622 - acc: 0.7413\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5415 - acc: 0.7461\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5298 - acc: 0.7619\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5383 - acc: 0.7706\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5238 - acc: 0.7647\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5126 - acc: 0.7806\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5225 - acc: 0.7659\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5379 - acc: 0.7603\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4988 - acc: 0.7980\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5371 - acc: 0.7606\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5326 - acc: 0.7537\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5263 - acc: 0.7608\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5140 - acc: 0.7840\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5176 - acc: 0.7760\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4883 - acc: 0.7800\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4812 - acc: 0.7920\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4781 - acc: 0.8001\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4825 - acc: 0.7998\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4831 - acc: 0.7827\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4543 - acc: 0.8141\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4471 - acc: 0.8080\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4668 - acc: 0.7888\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4820 - acc: 0.7930\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4861 - acc: 0.7982\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4616 - acc: 0.8011\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4653 - acc: 0.7933\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4734 - acc: 0.7946\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4551 - acc: 0.8154\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4703 - acc: 0.8111\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4371 - acc: 0.8281\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4626 - acc: 0.8094\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4594 - acc: 0.7971\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4626 - acc: 0.8129\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4447 - acc: 0.8237\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4306 - acc: 0.8331\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4537 - acc: 0.8154\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4245 - acc: 0.8238\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4337 - acc: 0.8114\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4356 - acc: 0.8326\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4184 - acc: 0.8280\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4429 - acc: 0.8240\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4444 - acc: 0.8215\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4051 - acc: 0.8400\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4584 - acc: 0.8065\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4110 - acc: 0.8378\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4234 - acc: 0.8414\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3866 - acc: 0.8494\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4359 - acc: 0.8183\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4102 - acc: 0.8440\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4124 - acc: 0.8311\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4391 - acc: 0.8061\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3938 - acc: 0.8468\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4065 - acc: 0.8450\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3838 - acc: 0.8516\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3850 - acc: 0.8560\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4109 - acc: 0.8307\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4136 - acc: 0.8333\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4079 - acc: 0.8357\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4079 - acc: 0.8445\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3893 - acc: 0.8341\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4094 - acc: 0.8410\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4205 - acc: 0.8192\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3758 - acc: 0.8427\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4039 - acc: 0.8353\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4080 - acc: 0.8335\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3888 - acc: 0.8471\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3906 - acc: 0.8412\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4033 - acc: 0.8401\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4027 - acc: 0.8460\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4022 - acc: 0.8422\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3938 - acc: 0.8419\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3995 - acc: 0.8359\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3846 - acc: 0.8490\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3632 - acc: 0.8525\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3786 - acc: 0.8479\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3794 - acc: 0.8508\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4085 - acc: 0.8471\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4001 - acc: 0.8228\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3880 - acc: 0.8455\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3691 - acc: 0.8577\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3741 - acc: 0.8432\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3848 - acc: 0.8461\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4054 - acc: 0.8350\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3561 - acc: 0.8484\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3835 - acc: 0.8447\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3686 - acc: 0.8528\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3892 - acc: 0.8382\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3970 - acc: 0.8375\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3009 - acc: 0.9000\n",
            "processing fold # 1\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 40ms/step - loss: 1.4210 - acc: 0.5083\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.9221 - acc: 0.5545\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.7403 - acc: 0.6044\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6831 - acc: 0.6635\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6546 - acc: 0.6955\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.6171 - acc: 0.7294\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5995 - acc: 0.7174\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5839 - acc: 0.7282\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5880 - acc: 0.7487\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5682 - acc: 0.7484\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5639 - acc: 0.7463\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5825 - acc: 0.7295\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5682 - acc: 0.7347\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5321 - acc: 0.7704\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5234 - acc: 0.7667\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5456 - acc: 0.7502\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5181 - acc: 0.7794\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4924 - acc: 0.7896\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5382 - acc: 0.7668\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.5019 - acc: 0.7770\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5070 - acc: 0.7865\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5123 - acc: 0.7823\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5052 - acc: 0.7952\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4982 - acc: 0.7882\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4941 - acc: 0.7896\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4973 - acc: 0.7897\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4720 - acc: 0.8038\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4766 - acc: 0.8022\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4850 - acc: 0.7991\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4833 - acc: 0.7804\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4409 - acc: 0.8147\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4745 - acc: 0.8044\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4657 - acc: 0.7953\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4242 - acc: 0.8157\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4448 - acc: 0.8108\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4450 - acc: 0.8218\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4489 - acc: 0.8145\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4329 - acc: 0.8146\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4504 - acc: 0.8117\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4307 - acc: 0.8330\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4146 - acc: 0.8407\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4471 - acc: 0.8186\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4263 - acc: 0.8298\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4251 - acc: 0.8450\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4320 - acc: 0.8204\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4483 - acc: 0.8256\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4310 - acc: 0.8297\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4318 - acc: 0.8319\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4209 - acc: 0.8169\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4129 - acc: 0.8548\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4048 - acc: 0.8361\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4372 - acc: 0.8197\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4313 - acc: 0.8266\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4148 - acc: 0.8318\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4163 - acc: 0.8341\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4128 - acc: 0.8229\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4327 - acc: 0.8357\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4242 - acc: 0.8235\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3837 - acc: 0.8453\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.3997 - acc: 0.8357\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4127 - acc: 0.8266\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 4s 39ms/step - loss: 0.4093 - acc: 0.8362\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3969 - acc: 0.8473\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4140 - acc: 0.8253\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3704 - acc: 0.8609\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4031 - acc: 0.8412\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4068 - acc: 0.8446\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3943 - acc: 0.8377\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3880 - acc: 0.8316\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3755 - acc: 0.8472\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3817 - acc: 0.8454\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4294 - acc: 0.8238\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4009 - acc: 0.8341\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3871 - acc: 0.8474\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3645 - acc: 0.8541\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3609 - acc: 0.8502\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3749 - acc: 0.8534\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3918 - acc: 0.8390\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3752 - acc: 0.8519\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3672 - acc: 0.8532\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3661 - acc: 0.8510\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3685 - acc: 0.8621\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3773 - acc: 0.8476\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3879 - acc: 0.8457\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3798 - acc: 0.8473\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3781 - acc: 0.8459\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3638 - acc: 0.8549\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3764 - acc: 0.8452\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3587 - acc: 0.8679\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3607 - acc: 0.8688\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3626 - acc: 0.8609\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3682 - acc: 0.8613\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3794 - acc: 0.8452\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3559 - acc: 0.8532\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3544 - acc: 0.8618\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3702 - acc: 0.8571\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3549 - acc: 0.8609\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3576 - acc: 0.8650\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3534 - acc: 0.8623\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3515 - acc: 0.8605\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3470 - acc: 0.8827\n",
            "processing fold # 2\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 41ms/step - loss: 1.4163 - acc: 0.5007\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.9041 - acc: 0.5542\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.7335 - acc: 0.5695\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6667 - acc: 0.6673\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6311 - acc: 0.6953\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6079 - acc: 0.7175\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6142 - acc: 0.7104\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5967 - acc: 0.7394\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5663 - acc: 0.7523\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5768 - acc: 0.7214\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5731 - acc: 0.7233\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5408 - acc: 0.7583\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5484 - acc: 0.7539\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5285 - acc: 0.7803\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5142 - acc: 0.7796\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5292 - acc: 0.7763\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5201 - acc: 0.7779\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5423 - acc: 0.7573\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5350 - acc: 0.7649\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5313 - acc: 0.7673\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5272 - acc: 0.7714\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5157 - acc: 0.7792\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4994 - acc: 0.7868\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5049 - acc: 0.7791\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5135 - acc: 0.7672\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4997 - acc: 0.7842\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4665 - acc: 0.8243\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4711 - acc: 0.7996\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4707 - acc: 0.8039\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4941 - acc: 0.7898\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4765 - acc: 0.8084\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4630 - acc: 0.8037\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4779 - acc: 0.8016\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4797 - acc: 0.8001\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4590 - acc: 0.8091\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4665 - acc: 0.8028\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4482 - acc: 0.8112\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4502 - acc: 0.8145\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4365 - acc: 0.8259\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4784 - acc: 0.8063\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4268 - acc: 0.8317\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4419 - acc: 0.8162\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4509 - acc: 0.8084\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4324 - acc: 0.8141\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4485 - acc: 0.8123\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4212 - acc: 0.8174\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4471 - acc: 0.8070\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4374 - acc: 0.8105\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4366 - acc: 0.8208\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4436 - acc: 0.8195\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4279 - acc: 0.8128\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4029 - acc: 0.8358\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4115 - acc: 0.8276\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4028 - acc: 0.8324\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4232 - acc: 0.8459\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4029 - acc: 0.8351\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4116 - acc: 0.8289\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4125 - acc: 0.8338\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4110 - acc: 0.8346\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4191 - acc: 0.8268\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3988 - acc: 0.8356\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4066 - acc: 0.8283\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3909 - acc: 0.8382\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4065 - acc: 0.8312\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4056 - acc: 0.8396\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3791 - acc: 0.8422\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3927 - acc: 0.8393\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4038 - acc: 0.8330\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3891 - acc: 0.8582\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3814 - acc: 0.8448\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.4217 - acc: 0.8322\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3721 - acc: 0.8605\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4026 - acc: 0.8347\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3866 - acc: 0.8423\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3705 - acc: 0.8492\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3866 - acc: 0.8471\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3761 - acc: 0.8494\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3991 - acc: 0.8327\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3966 - acc: 0.8346\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4063 - acc: 0.8382\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3981 - acc: 0.8392\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3521 - acc: 0.8609\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3870 - acc: 0.8353\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3751 - acc: 0.8588\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3592 - acc: 0.8650\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3719 - acc: 0.8486\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3582 - acc: 0.8579\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3670 - acc: 0.8442\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3770 - acc: 0.8581\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3700 - acc: 0.8497\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3693 - acc: 0.8599\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3519 - acc: 0.8587\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3624 - acc: 0.8599\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3386 - acc: 0.8676\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3610 - acc: 0.8482\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3647 - acc: 0.8507\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3616 - acc: 0.8568\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3686 - acc: 0.8504\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3334 - acc: 0.8705\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3585 - acc: 0.8633\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3684 - acc: 0.8712\n",
            "processing fold # 3\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 39ms/step - loss: 1.4157 - acc: 0.5269\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.9122 - acc: 0.5356\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.7338 - acc: 0.5920\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6748 - acc: 0.6731\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6440 - acc: 0.6877\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6003 - acc: 0.7410\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6068 - acc: 0.7326\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5878 - acc: 0.7445\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5654 - acc: 0.7564\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5699 - acc: 0.7473\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5538 - acc: 0.7553\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5626 - acc: 0.7445\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5372 - acc: 0.7704\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5591 - acc: 0.7356\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5284 - acc: 0.7786\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5365 - acc: 0.7639\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5183 - acc: 0.7757\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5086 - acc: 0.7702\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5326 - acc: 0.7728\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5619 - acc: 0.7475\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5269 - acc: 0.7628\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5159 - acc: 0.7743\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5067 - acc: 0.7770\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5211 - acc: 0.7796\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4973 - acc: 0.7913\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5088 - acc: 0.7697\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5064 - acc: 0.7950\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4545 - acc: 0.8142\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4473 - acc: 0.8208\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4825 - acc: 0.8127\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4719 - acc: 0.8094\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4846 - acc: 0.8014\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4596 - acc: 0.8165\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4699 - acc: 0.8025\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4576 - acc: 0.8201\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4452 - acc: 0.8137\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4648 - acc: 0.8098\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4563 - acc: 0.8098\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4669 - acc: 0.7992\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4336 - acc: 0.8186\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4572 - acc: 0.8124\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4506 - acc: 0.8157\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4470 - acc: 0.8267\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4546 - acc: 0.8260\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4360 - acc: 0.8170\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4415 - acc: 0.8310\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4141 - acc: 0.8320\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4328 - acc: 0.8395\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4071 - acc: 0.8380\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4169 - acc: 0.8291\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4343 - acc: 0.8324\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4307 - acc: 0.8259\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4433 - acc: 0.8181\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4004 - acc: 0.8462\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3992 - acc: 0.8548\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4091 - acc: 0.8402\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3983 - acc: 0.8446\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4260 - acc: 0.8296\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4068 - acc: 0.8398\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4053 - acc: 0.8441\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4132 - acc: 0.8379\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4051 - acc: 0.8341\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3917 - acc: 0.8388\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4091 - acc: 0.8317\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3938 - acc: 0.8432\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3963 - acc: 0.8438\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4175 - acc: 0.8267\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4259 - acc: 0.8294\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.4108 - acc: 0.8388\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3904 - acc: 0.8469\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3861 - acc: 0.8471\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3725 - acc: 0.8550\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3797 - acc: 0.8581\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3980 - acc: 0.8389\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3711 - acc: 0.8643\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3959 - acc: 0.8488\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4053 - acc: 0.8435\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3731 - acc: 0.8519\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3798 - acc: 0.8516\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3702 - acc: 0.8511\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3818 - acc: 0.8461\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3887 - acc: 0.8514\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3685 - acc: 0.8462\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3616 - acc: 0.8656\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3686 - acc: 0.8640\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3777 - acc: 0.8547\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3648 - acc: 0.8594\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3606 - acc: 0.8581\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4001 - acc: 0.8288\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.3523 - acc: 0.8654\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3750 - acc: 0.8563\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3710 - acc: 0.8489\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.3624 - acc: 0.8591\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3705 - acc: 0.8573\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3936 - acc: 0.8416\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3785 - acc: 0.8469\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3542 - acc: 0.8583\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3400 - acc: 0.8738\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3763 - acc: 0.8518\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.3580 - acc: 0.8548\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3025 - acc: 0.8904\n",
            "processing fold # 4\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 1.4081 - acc: 0.5130\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.9158 - acc: 0.5294\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.7368 - acc: 0.6359\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.6787 - acc: 0.6869\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.6182 - acc: 0.7326\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.6113 - acc: 0.7347\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5851 - acc: 0.7546\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.5986 - acc: 0.7251\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.5626 - acc: 0.7485\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.5815 - acc: 0.7489\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5406 - acc: 0.7683\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5412 - acc: 0.7695\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 4s 40ms/step - loss: 0.5437 - acc: 0.7561\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5425 - acc: 0.7592\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5294 - acc: 0.7673\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.5153 - acc: 0.7737\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.5429 - acc: 0.7745\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5252 - acc: 0.7764\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.5094 - acc: 0.7863\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5161 - acc: 0.7852\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5231 - acc: 0.7669\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5133 - acc: 0.7775\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4878 - acc: 0.7914\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4618 - acc: 0.8192\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 4s 42ms/step - loss: 0.4705 - acc: 0.7993\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.5014 - acc: 0.7938\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4865 - acc: 0.8017\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4629 - acc: 0.8041\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 4s 41ms/step - loss: 0.4557 - acc: 0.8153\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4616 - acc: 0.8109\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5081 - acc: 0.7759\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4437 - acc: 0.8262\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4284 - acc: 0.8174\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4420 - acc: 0.8208\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4399 - acc: 0.8244\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4714 - acc: 0.8099\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4647 - acc: 0.8061\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4308 - acc: 0.8304\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4233 - acc: 0.8268\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4337 - acc: 0.8266\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4489 - acc: 0.8156\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4361 - acc: 0.8177\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4405 - acc: 0.8117\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4332 - acc: 0.8366\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4478 - acc: 0.8160\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4243 - acc: 0.8322\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4086 - acc: 0.8392\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4175 - acc: 0.8301\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4263 - acc: 0.8288\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4247 - acc: 0.8387\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3994 - acc: 0.8504\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.4034 - acc: 0.8343\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4113 - acc: 0.8424\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3978 - acc: 0.8451\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4010 - acc: 0.8473\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4065 - acc: 0.8271\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4199 - acc: 0.8323\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3972 - acc: 0.8466\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4158 - acc: 0.8500\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3975 - acc: 0.8379\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3923 - acc: 0.8435\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4074 - acc: 0.8467\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3679 - acc: 0.8596\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4035 - acc: 0.8409\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3784 - acc: 0.8525\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3803 - acc: 0.8521\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.4126 - acc: 0.8266\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3697 - acc: 0.8544\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3920 - acc: 0.8379\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3779 - acc: 0.8424\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3810 - acc: 0.8460\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3680 - acc: 0.8552\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.3673 - acc: 0.8542\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.3449 - acc: 0.8721\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3574 - acc: 0.8584\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3736 - acc: 0.8437\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3662 - acc: 0.8612\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3669 - acc: 0.8591\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3811 - acc: 0.8395\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3503 - acc: 0.8593\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3504 - acc: 0.8729\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3787 - acc: 0.8506\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3426 - acc: 0.8617\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.3681 - acc: 0.8517\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.3679 - acc: 0.8586\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3750 - acc: 0.8393\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3527 - acc: 0.8591\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3588 - acc: 0.8571\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3558 - acc: 0.8721\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3680 - acc: 0.8519\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3494 - acc: 0.8773\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3589 - acc: 0.8573\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3282 - acc: 0.8712\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3598 - acc: 0.8575\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3427 - acc: 0.8740\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.3540 - acc: 0.8600\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3826 - acc: 0.8466\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.3568 - acc: 0.8646\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.3532 - acc: 0.8631\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.3616 - acc: 0.8632\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.4035 - acc: 0.8442\n",
            "-----------------------------------------------------\n",
            "batch_size: \t 20\n",
            "num_epochs: \t 100\n",
            "units_per_layer: [32  64  128  128]\n",
            "optimizer: \t Adam\n",
            "learning_rate: \t 0.01\n",
            "num_folds CV: \t 5\n",
            "-----------------------------------------------------\n",
            "processing fold # 0\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 6s 45ms/step - loss: 1.4045 - acc: 0.5450\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.9046 - acc: 0.5529\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.7372 - acc: 0.5631\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.6611 - acc: 0.6809\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.6270 - acc: 0.7096\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.5934 - acc: 0.7330\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5766 - acc: 0.7279\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.6137 - acc: 0.7211\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.5716 - acc: 0.7482\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5569 - acc: 0.7492\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.5712 - acc: 0.7257\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.5630 - acc: 0.7271\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5523 - acc: 0.7592\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5430 - acc: 0.7428\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5796 - acc: 0.7484\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5456 - acc: 0.7600\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5345 - acc: 0.7681\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5251 - acc: 0.7735\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.5303 - acc: 0.7640\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5144 - acc: 0.7560\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5054 - acc: 0.7838\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.5268 - acc: 0.7671\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5066 - acc: 0.7866\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.4982 - acc: 0.7853\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.5268 - acc: 0.7777\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.5116 - acc: 0.7747\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.4892 - acc: 0.7956\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.4776 - acc: 0.7902\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.4993 - acc: 0.7910\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.5001 - acc: 0.7954\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 49ms/step - loss: 0.4926 - acc: 0.8045\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.4697 - acc: 0.8043\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.4541 - acc: 0.8131\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 50ms/step - loss: 0.4516 - acc: 0.8064\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 51ms/step - loss: 0.4573 - acc: 0.8181\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.4562 - acc: 0.8173\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.4642 - acc: 0.8079\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.4432 - acc: 0.8168\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 49ms/step - loss: 0.4526 - acc: 0.8105\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 48ms/step - loss: 0.4384 - acc: 0.8271\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 47ms/step - loss: 0.4508 - acc: 0.8103\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 49ms/step - loss: 0.4432 - acc: 0.8150\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4462 - acc: 0.8103\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4418 - acc: 0.8115\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4736 - acc: 0.8159\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4515 - acc: 0.8134\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4235 - acc: 0.8183\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4249 - acc: 0.8350\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4245 - acc: 0.8297\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4303 - acc: 0.8214\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4341 - acc: 0.8147\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4606 - acc: 0.8206\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4271 - acc: 0.8338\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3895 - acc: 0.8612\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4261 - acc: 0.8272\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4195 - acc: 0.8272\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4080 - acc: 0.8347\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3999 - acc: 0.8454\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3814 - acc: 0.8405\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4402 - acc: 0.8124\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4225 - acc: 0.8156\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4058 - acc: 0.8455\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4075 - acc: 0.8275\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3794 - acc: 0.8381\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4188 - acc: 0.8320\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3854 - acc: 0.8338\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4120 - acc: 0.8267\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3909 - acc: 0.8459\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4024 - acc: 0.8389\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3966 - acc: 0.8360\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3814 - acc: 0.8414\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3724 - acc: 0.8578\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4182 - acc: 0.8257\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3842 - acc: 0.8506\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3958 - acc: 0.8310\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3880 - acc: 0.8535\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3705 - acc: 0.8548\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3705 - acc: 0.8463\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3872 - acc: 0.8411\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3753 - acc: 0.8441\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3992 - acc: 0.8376\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3871 - acc: 0.8466\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3877 - acc: 0.8426\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3780 - acc: 0.8495\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3652 - acc: 0.8589\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3685 - acc: 0.8612\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3761 - acc: 0.8650\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3834 - acc: 0.8575\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3727 - acc: 0.8474\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3835 - acc: 0.8348\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3708 - acc: 0.8512\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3590 - acc: 0.8566\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3668 - acc: 0.8478\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3748 - acc: 0.8497\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3616 - acc: 0.8504\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3762 - acc: 0.8414\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3583 - acc: 0.8643\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3525 - acc: 0.8472\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3642 - acc: 0.8640\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3503 - acc: 0.8629\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3130 - acc: 0.8865\n",
            "processing fold # 1\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 1.4081 - acc: 0.5354\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.9116 - acc: 0.5433\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.7348 - acc: 0.5876\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6797 - acc: 0.6395\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6288 - acc: 0.7291\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6093 - acc: 0.7331\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6076 - acc: 0.7098\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5847 - acc: 0.7254\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5605 - acc: 0.7439\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5897 - acc: 0.7177\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5648 - acc: 0.7544\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5684 - acc: 0.7411\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5492 - acc: 0.7681\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5343 - acc: 0.7550\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5455 - acc: 0.7368\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5501 - acc: 0.7622\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5502 - acc: 0.7449\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5514 - acc: 0.7483\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5277 - acc: 0.7675\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5228 - acc: 0.7735\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5147 - acc: 0.7753\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4979 - acc: 0.7694\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5095 - acc: 0.7730\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5290 - acc: 0.7702\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5066 - acc: 0.7696\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4791 - acc: 0.7881\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4954 - acc: 0.8021\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4759 - acc: 0.7953\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4668 - acc: 0.8013\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4727 - acc: 0.7913\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4627 - acc: 0.8060\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4713 - acc: 0.8014\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4598 - acc: 0.8106\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4829 - acc: 0.7769\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4805 - acc: 0.7867\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4486 - acc: 0.8135\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4596 - acc: 0.8088\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4396 - acc: 0.8198\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4392 - acc: 0.8217\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4413 - acc: 0.8200\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4391 - acc: 0.8150\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4578 - acc: 0.8097\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4223 - acc: 0.8245\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4656 - acc: 0.8061\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4288 - acc: 0.8114\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4502 - acc: 0.8290\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4418 - acc: 0.8242\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4272 - acc: 0.8159\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4433 - acc: 0.8093\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4400 - acc: 0.8094\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4318 - acc: 0.8370\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4030 - acc: 0.8440\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4207 - acc: 0.8331\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4167 - acc: 0.8332\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4361 - acc: 0.8157\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4028 - acc: 0.8495\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4188 - acc: 0.8334\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4166 - acc: 0.8350\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4419 - acc: 0.8202\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4102 - acc: 0.8248\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4060 - acc: 0.8410\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3959 - acc: 0.8494\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4005 - acc: 0.8443\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3940 - acc: 0.8372\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3905 - acc: 0.8413\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4170 - acc: 0.8271\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4046 - acc: 0.8450\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4158 - acc: 0.8304\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3759 - acc: 0.8529\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4181 - acc: 0.8334\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3873 - acc: 0.8352\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3682 - acc: 0.8516\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3696 - acc: 0.8535\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3739 - acc: 0.8504\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3801 - acc: 0.8474\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3843 - acc: 0.8326\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3730 - acc: 0.8621\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3756 - acc: 0.8323\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3851 - acc: 0.8404\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3881 - acc: 0.8552\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3958 - acc: 0.8442\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3791 - acc: 0.8484\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3641 - acc: 0.8581\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3459 - acc: 0.8611\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3863 - acc: 0.8473\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3941 - acc: 0.8362\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3882 - acc: 0.8407\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3754 - acc: 0.8466\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3594 - acc: 0.8579\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3728 - acc: 0.8567\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3634 - acc: 0.8592\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3606 - acc: 0.8544\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3624 - acc: 0.8570\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3566 - acc: 0.8422\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3531 - acc: 0.8554\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3330 - acc: 0.8716\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3494 - acc: 0.8659\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3579 - acc: 0.8634\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3725 - acc: 0.8455\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3472 - acc: 0.8613\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3910 - acc: 0.8269\n",
            "processing fold # 2\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 1.4204 - acc: 0.5202\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.9268 - acc: 0.5640\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.7416 - acc: 0.6183\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6819 - acc: 0.6730\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6455 - acc: 0.7220\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6263 - acc: 0.7163\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6185 - acc: 0.7131\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5992 - acc: 0.7425\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6099 - acc: 0.7227\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5688 - acc: 0.7511\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5524 - acc: 0.7638\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5623 - acc: 0.7447\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5648 - acc: 0.7481\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5633 - acc: 0.7567\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5394 - acc: 0.7600\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5475 - acc: 0.7568\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5498 - acc: 0.7450\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5186 - acc: 0.7820\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5028 - acc: 0.7871\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5217 - acc: 0.7701\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5490 - acc: 0.7615\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4738 - acc: 0.8090\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5105 - acc: 0.7682\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5035 - acc: 0.7817\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4901 - acc: 0.7984\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4852 - acc: 0.7797\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4730 - acc: 0.8100\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4873 - acc: 0.7985\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4618 - acc: 0.7940\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4752 - acc: 0.8228\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5090 - acc: 0.7836\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5072 - acc: 0.7864\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4632 - acc: 0.8071\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4706 - acc: 0.8034\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4518 - acc: 0.8074\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4708 - acc: 0.8079\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4729 - acc: 0.7989\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4358 - acc: 0.8116\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4379 - acc: 0.8131\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4504 - acc: 0.8114\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4588 - acc: 0.8106\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4178 - acc: 0.8310\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4344 - acc: 0.8390\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4351 - acc: 0.8232\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4579 - acc: 0.8194\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4133 - acc: 0.8376\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4533 - acc: 0.8073\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4301 - acc: 0.8261\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4345 - acc: 0.8167\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4229 - acc: 0.8273\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4168 - acc: 0.8411\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4194 - acc: 0.8328\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4324 - acc: 0.8237\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4056 - acc: 0.8416\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4273 - acc: 0.8160\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4096 - acc: 0.8347\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4258 - acc: 0.8444\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4326 - acc: 0.8284\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4166 - acc: 0.8355\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4074 - acc: 0.8460\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4136 - acc: 0.8338\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4118 - acc: 0.8400\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4032 - acc: 0.8499\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4022 - acc: 0.8459\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3994 - acc: 0.8332\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3949 - acc: 0.8497\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3929 - acc: 0.8376\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3960 - acc: 0.8457\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4219 - acc: 0.8164\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4247 - acc: 0.8354\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3997 - acc: 0.8393\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3724 - acc: 0.8695\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4087 - acc: 0.8370\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3979 - acc: 0.8337\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3833 - acc: 0.8398\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3900 - acc: 0.8454\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3935 - acc: 0.8366\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3881 - acc: 0.8435\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3800 - acc: 0.8536\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4087 - acc: 0.8323\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3677 - acc: 0.8545\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3578 - acc: 0.8564\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3910 - acc: 0.8475\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3677 - acc: 0.8636\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3587 - acc: 0.8570\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3782 - acc: 0.8464\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3962 - acc: 0.8329\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3760 - acc: 0.8442\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3626 - acc: 0.8632\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3696 - acc: 0.8519\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3477 - acc: 0.8694\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3533 - acc: 0.8532\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3856 - acc: 0.8559\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3763 - acc: 0.8552\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3654 - acc: 0.8479\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3421 - acc: 0.8682\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3659 - acc: 0.8599\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3459 - acc: 0.8666\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3582 - acc: 0.8428\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3510 - acc: 0.8513\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2901 - acc: 0.8904\n",
            "processing fold # 3\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 1.4324 - acc: 0.5262\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.9360 - acc: 0.5520\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.7570 - acc: 0.5960\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.7067 - acc: 0.6454\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6611 - acc: 0.6930\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6360 - acc: 0.7080\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6035 - acc: 0.7304\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5752 - acc: 0.7628\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6016 - acc: 0.7334\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5697 - acc: 0.7560\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5565 - acc: 0.7619\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5591 - acc: 0.7560\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5716 - acc: 0.7553\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5570 - acc: 0.7623\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5468 - acc: 0.7698\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5507 - acc: 0.7603\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5312 - acc: 0.7742\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5473 - acc: 0.7523\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5479 - acc: 0.7666\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5606 - acc: 0.7619\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5449 - acc: 0.7375\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5281 - acc: 0.7692\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5338 - acc: 0.7806\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5024 - acc: 0.7904\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5023 - acc: 0.7810\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4932 - acc: 0.7964\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5095 - acc: 0.7813\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5043 - acc: 0.7854\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4851 - acc: 0.7739\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4975 - acc: 0.7969\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4615 - acc: 0.8048\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4623 - acc: 0.8168\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4621 - acc: 0.8112\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4797 - acc: 0.7890\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4504 - acc: 0.8227\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4720 - acc: 0.8016\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4456 - acc: 0.8188\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4560 - acc: 0.8142\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4832 - acc: 0.8035\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4437 - acc: 0.8364\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4556 - acc: 0.7994\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4415 - acc: 0.8307\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4459 - acc: 0.8256\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4305 - acc: 0.8339\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4421 - acc: 0.8254\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4322 - acc: 0.8194\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4699 - acc: 0.8064\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4581 - acc: 0.8113\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4282 - acc: 0.8284\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4444 - acc: 0.8216\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4404 - acc: 0.8172\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4237 - acc: 0.8267\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4347 - acc: 0.8181\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4365 - acc: 0.8180\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4278 - acc: 0.8234\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4209 - acc: 0.8405\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4319 - acc: 0.8204\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4436 - acc: 0.8108\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4419 - acc: 0.8347\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4084 - acc: 0.8451\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4457 - acc: 0.8140\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4194 - acc: 0.8313\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3992 - acc: 0.8453\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3985 - acc: 0.8433\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3818 - acc: 0.8554\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3943 - acc: 0.8552\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3989 - acc: 0.8414\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4165 - acc: 0.8353\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4152 - acc: 0.8380\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4163 - acc: 0.8305\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4160 - acc: 0.8326\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3845 - acc: 0.8538\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3984 - acc: 0.8316\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4012 - acc: 0.8422\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3760 - acc: 0.8459\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3848 - acc: 0.8391\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3963 - acc: 0.8387\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3924 - acc: 0.8548\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3959 - acc: 0.8315\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3795 - acc: 0.8481\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3945 - acc: 0.8397\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3903 - acc: 0.8454\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3766 - acc: 0.8510\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3851 - acc: 0.8373\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3902 - acc: 0.8449\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3755 - acc: 0.8524\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3801 - acc: 0.8485\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3838 - acc: 0.8555\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3665 - acc: 0.8576\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3998 - acc: 0.8406\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3812 - acc: 0.8503\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3668 - acc: 0.8592\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3957 - acc: 0.8447\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3860 - acc: 0.8570\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3749 - acc: 0.8507\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3990 - acc: 0.8340\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3722 - acc: 0.8560\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3611 - acc: 0.8643\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3818 - acc: 0.8474\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3657 - acc: 0.8550\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2653 - acc: 0.9058\n",
            "processing fold # 4\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 1.4021 - acc: 0.5338\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.9069 - acc: 0.5385\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.7275 - acc: 0.6036\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6801 - acc: 0.6378\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6284 - acc: 0.7062\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5991 - acc: 0.7229\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5834 - acc: 0.7488\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5856 - acc: 0.7321\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5806 - acc: 0.7412\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5647 - acc: 0.7479\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5516 - acc: 0.7375\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5515 - acc: 0.7577\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5613 - acc: 0.7531\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5472 - acc: 0.7631\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5271 - acc: 0.7807\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5313 - acc: 0.7648\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5457 - acc: 0.7689\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5324 - acc: 0.7668\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5319 - acc: 0.7598\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4901 - acc: 0.7861\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5228 - acc: 0.7647\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4993 - acc: 0.7941\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5148 - acc: 0.7807\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4923 - acc: 0.7783\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4938 - acc: 0.8058\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4823 - acc: 0.7959\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5120 - acc: 0.7764\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4822 - acc: 0.7869\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4746 - acc: 0.7989\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4795 - acc: 0.7917\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4647 - acc: 0.8149\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4615 - acc: 0.8061\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4504 - acc: 0.8162\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4848 - acc: 0.7993\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4647 - acc: 0.8067\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4544 - acc: 0.8160\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4505 - acc: 0.8145\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4608 - acc: 0.8067\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4408 - acc: 0.8198\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4502 - acc: 0.8110\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4526 - acc: 0.8051\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4313 - acc: 0.8157\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4600 - acc: 0.8206\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4470 - acc: 0.8152\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4381 - acc: 0.8164\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4247 - acc: 0.8328\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4452 - acc: 0.8230\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4528 - acc: 0.8012\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4191 - acc: 0.8243\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4265 - acc: 0.8312\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4077 - acc: 0.8312\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4354 - acc: 0.8214\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4316 - acc: 0.8240\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4221 - acc: 0.8272\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4111 - acc: 0.8430\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4246 - acc: 0.8345\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3863 - acc: 0.8477\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4082 - acc: 0.8243\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4154 - acc: 0.8365\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3865 - acc: 0.8395\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4127 - acc: 0.8385\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4065 - acc: 0.8384\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3700 - acc: 0.8475\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4167 - acc: 0.8412\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4189 - acc: 0.8367\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3929 - acc: 0.8333\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3839 - acc: 0.8432\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3880 - acc: 0.8371\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3868 - acc: 0.8572\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3942 - acc: 0.8368\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4088 - acc: 0.8251\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3815 - acc: 0.8467\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3860 - acc: 0.8559\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3900 - acc: 0.8277\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3665 - acc: 0.8506\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3848 - acc: 0.8520\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3744 - acc: 0.8517\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3523 - acc: 0.8605\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3766 - acc: 0.8511\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3611 - acc: 0.8596\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3498 - acc: 0.8600\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3528 - acc: 0.8629\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3790 - acc: 0.8534\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3760 - acc: 0.8433\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3661 - acc: 0.8548\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3658 - acc: 0.8667\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3544 - acc: 0.8583\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3531 - acc: 0.8587\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3790 - acc: 0.8645\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3562 - acc: 0.8584\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3464 - acc: 0.8690\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3357 - acc: 0.8658\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3342 - acc: 0.8645\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3638 - acc: 0.8562\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3567 - acc: 0.8702\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3717 - acc: 0.8570\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3658 - acc: 0.8539\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3596 - acc: 0.8586\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3628 - acc: 0.8600\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3475 - acc: 0.8435\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3397 - acc: 0.8635\n",
            "-----------------------------------------------------\n",
            "batch_size: \t 20\n",
            "num_epochs: \t 100\n",
            "units_per_layer: [32  64  128  256]\n",
            "optimizer: \t Adam\n",
            "learning_rate: \t 0.01\n",
            "num_folds CV: \t 5\n",
            "-----------------------------------------------------\n",
            "processing fold # 0\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 1.4202 - acc: 0.5207\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.9219 - acc: 0.5772\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.7441 - acc: 0.6279\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6689 - acc: 0.6853\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6377 - acc: 0.7345\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6229 - acc: 0.7200\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5694 - acc: 0.7499\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5865 - acc: 0.7500\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5914 - acc: 0.7292\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5602 - acc: 0.7666\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5580 - acc: 0.7519\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5512 - acc: 0.7732\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5800 - acc: 0.7329\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5521 - acc: 0.7537\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5178 - acc: 0.7785\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5455 - acc: 0.7554\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5407 - acc: 0.7626\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5477 - acc: 0.7556\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5418 - acc: 0.7428\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5088 - acc: 0.7844\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5471 - acc: 0.7605\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5386 - acc: 0.7564\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4985 - acc: 0.7808\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5039 - acc: 0.7701\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4904 - acc: 0.7993\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4714 - acc: 0.7912\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5186 - acc: 0.7632\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4925 - acc: 0.7848\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4828 - acc: 0.7847\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4886 - acc: 0.7838\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4853 - acc: 0.7891\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4910 - acc: 0.7954\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4856 - acc: 0.7823\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4302 - acc: 0.8201\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4870 - acc: 0.7886\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4706 - acc: 0.7927\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4689 - acc: 0.8046\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4977 - acc: 0.7857\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4535 - acc: 0.8211\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4704 - acc: 0.7887\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4572 - acc: 0.7989\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4607 - acc: 0.8097\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4485 - acc: 0.8108\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4525 - acc: 0.8213\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4730 - acc: 0.8083\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4507 - acc: 0.8179\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4536 - acc: 0.8094\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4335 - acc: 0.8291\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4559 - acc: 0.8086\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4127 - acc: 0.8356\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4288 - acc: 0.8193\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4451 - acc: 0.8255\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4327 - acc: 0.8305\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4278 - acc: 0.8271\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4314 - acc: 0.8325\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4356 - acc: 0.8208\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4287 - acc: 0.8326\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4282 - acc: 0.8345\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4251 - acc: 0.8373\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4243 - acc: 0.8234\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4353 - acc: 0.8196\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4308 - acc: 0.8236\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4180 - acc: 0.8199\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4061 - acc: 0.8354\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4133 - acc: 0.8284\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4196 - acc: 0.8258\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4145 - acc: 0.8291\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4178 - acc: 0.8459\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4107 - acc: 0.8271\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4200 - acc: 0.8409\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4016 - acc: 0.8315\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4102 - acc: 0.8373\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4001 - acc: 0.8395\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4057 - acc: 0.8407\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3813 - acc: 0.8548\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3972 - acc: 0.8401\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3897 - acc: 0.8435\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3861 - acc: 0.8424\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4196 - acc: 0.8335\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3804 - acc: 0.8437\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3956 - acc: 0.8347\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3976 - acc: 0.8399\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4106 - acc: 0.8347\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3778 - acc: 0.8443\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3948 - acc: 0.8431\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3753 - acc: 0.8575\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3865 - acc: 0.8470\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3719 - acc: 0.8592\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3566 - acc: 0.8620\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3852 - acc: 0.8530\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3610 - acc: 0.8595\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3879 - acc: 0.8479\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3988 - acc: 0.8460\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3699 - acc: 0.8524\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3749 - acc: 0.8541\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3790 - acc: 0.8354\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3488 - acc: 0.8595\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3885 - acc: 0.8448\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3907 - acc: 0.8387\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4073 - acc: 0.8334\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2901 - acc: 0.8885\n",
            "processing fold # 1\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 1.4160 - acc: 0.5094\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.9233 - acc: 0.5362\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.7452 - acc: 0.5897\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6931 - acc: 0.6215\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6526 - acc: 0.6985\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6106 - acc: 0.7210\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6012 - acc: 0.7193\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6019 - acc: 0.7262\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5892 - acc: 0.7161\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5619 - acc: 0.7513\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5652 - acc: 0.7393\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5715 - acc: 0.7471\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5550 - acc: 0.7523\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5210 - acc: 0.7749\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5393 - acc: 0.7616\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5169 - acc: 0.7794\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5349 - acc: 0.7605\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5256 - acc: 0.7698\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5393 - acc: 0.7630\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5032 - acc: 0.7642\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4930 - acc: 0.7930\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5254 - acc: 0.7583\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5076 - acc: 0.7674\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4922 - acc: 0.7766\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5000 - acc: 0.7859\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4972 - acc: 0.7860\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4749 - acc: 0.8082\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4955 - acc: 0.7747\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4619 - acc: 0.8143\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4704 - acc: 0.7927\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4856 - acc: 0.8062\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4564 - acc: 0.8092\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4889 - acc: 0.7893\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4659 - acc: 0.7967\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4691 - acc: 0.8026\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4830 - acc: 0.8028\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4464 - acc: 0.8195\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4411 - acc: 0.8195\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4319 - acc: 0.8217\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4566 - acc: 0.8063\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4481 - acc: 0.8087\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4584 - acc: 0.8061\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4176 - acc: 0.8336\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4570 - acc: 0.8109\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4676 - acc: 0.7975\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4515 - acc: 0.8192\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4411 - acc: 0.8219\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4289 - acc: 0.8146\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4557 - acc: 0.8226\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4327 - acc: 0.8289\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4278 - acc: 0.8248\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4263 - acc: 0.8287\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4409 - acc: 0.8225\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4389 - acc: 0.8196\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4281 - acc: 0.8309\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4332 - acc: 0.8098\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4418 - acc: 0.8136\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4155 - acc: 0.8273\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4274 - acc: 0.8231\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3890 - acc: 0.8434\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4198 - acc: 0.8318\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4327 - acc: 0.8323\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3952 - acc: 0.8392\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4090 - acc: 0.8364\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4399 - acc: 0.8189\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3909 - acc: 0.8435\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4113 - acc: 0.8243\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4015 - acc: 0.8363\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4127 - acc: 0.8310\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3932 - acc: 0.8474\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4007 - acc: 0.8390\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3852 - acc: 0.8496\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4094 - acc: 0.8388\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4130 - acc: 0.8185\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4063 - acc: 0.8211\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3872 - acc: 0.8407\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3988 - acc: 0.8386\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3748 - acc: 0.8460\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3835 - acc: 0.8424\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3826 - acc: 0.8571\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3934 - acc: 0.8472\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4000 - acc: 0.8437\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3820 - acc: 0.8448\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3829 - acc: 0.8475\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3625 - acc: 0.8621\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3741 - acc: 0.8551\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3704 - acc: 0.8564\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3892 - acc: 0.8423\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3647 - acc: 0.8544\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3643 - acc: 0.8610\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3963 - acc: 0.8417\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3674 - acc: 0.8625\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3768 - acc: 0.8485\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3775 - acc: 0.8525\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3756 - acc: 0.8505\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3834 - acc: 0.8424\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3656 - acc: 0.8477\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3588 - acc: 0.8694\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3503 - acc: 0.8604\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3479 - acc: 0.8520\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3586 - acc: 0.8577\n",
            "processing fold # 2\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 1.4270 - acc: 0.4987\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.9206 - acc: 0.5742\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.7547 - acc: 0.6011\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6990 - acc: 0.6393\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6386 - acc: 0.7030\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6325 - acc: 0.7206\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6118 - acc: 0.7076\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5988 - acc: 0.7245\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5709 - acc: 0.7582\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5886 - acc: 0.7326\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5656 - acc: 0.7512\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5669 - acc: 0.7585\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5690 - acc: 0.7460\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5578 - acc: 0.7547\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5371 - acc: 0.7591\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5430 - acc: 0.7453\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5607 - acc: 0.7606\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5272 - acc: 0.7676\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5377 - acc: 0.7591\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5086 - acc: 0.7743\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5340 - acc: 0.7762\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5205 - acc: 0.7718\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4960 - acc: 0.7913\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5048 - acc: 0.7760\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4872 - acc: 0.7854\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5173 - acc: 0.7607\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5050 - acc: 0.7866\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4975 - acc: 0.7904\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4936 - acc: 0.7842\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4835 - acc: 0.7943\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5027 - acc: 0.7888\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4568 - acc: 0.8047\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4774 - acc: 0.8037\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4643 - acc: 0.7956\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4739 - acc: 0.8039\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4534 - acc: 0.8034\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4604 - acc: 0.8006\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4699 - acc: 0.7971\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4904 - acc: 0.7920\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4627 - acc: 0.8134\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4618 - acc: 0.8077\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4695 - acc: 0.8017\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4616 - acc: 0.8188\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4539 - acc: 0.8134\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4609 - acc: 0.8038\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4399 - acc: 0.8187\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4504 - acc: 0.8106\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4285 - acc: 0.8171\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4529 - acc: 0.8120\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4803 - acc: 0.7864\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4576 - acc: 0.8056\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4389 - acc: 0.8190\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4517 - acc: 0.8150\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4348 - acc: 0.8216\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4321 - acc: 0.8226\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4273 - acc: 0.8272\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4338 - acc: 0.8224\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4373 - acc: 0.8100\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4309 - acc: 0.8186\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4379 - acc: 0.8198\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3986 - acc: 0.8532\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4451 - acc: 0.8297\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4315 - acc: 0.8334\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4426 - acc: 0.8286\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4173 - acc: 0.8409\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4105 - acc: 0.8376\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3984 - acc: 0.8491\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4126 - acc: 0.8412\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4200 - acc: 0.8359\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3983 - acc: 0.8361\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4152 - acc: 0.8332\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4109 - acc: 0.8293\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3871 - acc: 0.8474\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4233 - acc: 0.8436\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4008 - acc: 0.8452\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3958 - acc: 0.8468\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3961 - acc: 0.8379\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4083 - acc: 0.8306\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3895 - acc: 0.8496\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4165 - acc: 0.8372\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4061 - acc: 0.8288\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4012 - acc: 0.8305\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3985 - acc: 0.8391\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3729 - acc: 0.8508\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4085 - acc: 0.8340\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3848 - acc: 0.8424\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3854 - acc: 0.8446\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3831 - acc: 0.8355\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3666 - acc: 0.8661\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3840 - acc: 0.8495\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3800 - acc: 0.8378\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3747 - acc: 0.8543\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3707 - acc: 0.8535\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3679 - acc: 0.8610\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3636 - acc: 0.8620\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3692 - acc: 0.8495\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3477 - acc: 0.8612\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3593 - acc: 0.8673\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3893 - acc: 0.8474\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3716 - acc: 0.8475\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3627 - acc: 0.8519\n",
            "processing fold # 3\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 1.3995 - acc: 0.5051\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.8919 - acc: 0.5592\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.7326 - acc: 0.5853\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6896 - acc: 0.6191\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6451 - acc: 0.6685\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6126 - acc: 0.7140\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6144 - acc: 0.7173\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6133 - acc: 0.7190\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6021 - acc: 0.7257\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5400 - acc: 0.7660\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5544 - acc: 0.7524\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5554 - acc: 0.7650\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5486 - acc: 0.7591\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5642 - acc: 0.7306\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5474 - acc: 0.7534\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5579 - acc: 0.7509\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5266 - acc: 0.7566\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5420 - acc: 0.7495\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5332 - acc: 0.7565\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5239 - acc: 0.7428\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5205 - acc: 0.7578\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5214 - acc: 0.7704\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5165 - acc: 0.7652\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5171 - acc: 0.7573\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5150 - acc: 0.7877\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5028 - acc: 0.7765\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5120 - acc: 0.7712\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5290 - acc: 0.7688\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5098 - acc: 0.7719\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4792 - acc: 0.7884\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4894 - acc: 0.7928\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4844 - acc: 0.7936\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4666 - acc: 0.8205\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4879 - acc: 0.7850\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4962 - acc: 0.7745\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4774 - acc: 0.7973\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4707 - acc: 0.7899\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4936 - acc: 0.7770\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4595 - acc: 0.8059\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4666 - acc: 0.8015\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4722 - acc: 0.8024\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4566 - acc: 0.8069\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4682 - acc: 0.7997\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4512 - acc: 0.8077\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4494 - acc: 0.8049\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4501 - acc: 0.8017\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4578 - acc: 0.8127\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4343 - acc: 0.8269\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4383 - acc: 0.8266\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4515 - acc: 0.8060\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4562 - acc: 0.8144\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.4520 - acc: 0.8267\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4342 - acc: 0.8295\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4460 - acc: 0.8042\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4568 - acc: 0.8009\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4551 - acc: 0.8093\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4272 - acc: 0.8282\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4204 - acc: 0.8289\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4038 - acc: 0.8308\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4436 - acc: 0.8098\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4598 - acc: 0.8112\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4351 - acc: 0.8321\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4428 - acc: 0.8061\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4262 - acc: 0.8218\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3857 - acc: 0.8512\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4091 - acc: 0.8249\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4344 - acc: 0.8236\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4036 - acc: 0.8456\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4198 - acc: 0.8332\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4168 - acc: 0.8268\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4184 - acc: 0.8216\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3825 - acc: 0.8529\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3973 - acc: 0.8342\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4315 - acc: 0.8236\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3929 - acc: 0.8458\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3943 - acc: 0.8390\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4069 - acc: 0.8272\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4156 - acc: 0.8257\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4015 - acc: 0.8463\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3861 - acc: 0.8457\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4229 - acc: 0.8201\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3833 - acc: 0.8342\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4032 - acc: 0.8350\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3936 - acc: 0.8455\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3954 - acc: 0.8346\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4009 - acc: 0.8359\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3745 - acc: 0.8576\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3674 - acc: 0.8528\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3841 - acc: 0.8484\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4032 - acc: 0.8390\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3927 - acc: 0.8453\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3958 - acc: 0.8462\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 46ms/step - loss: 0.3882 - acc: 0.8482\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3759 - acc: 0.8362\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.3611 - acc: 0.8558\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3903 - acc: 0.8544\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3709 - acc: 0.8570\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3638 - acc: 0.8556\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3787 - acc: 0.8425\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3706 - acc: 0.8540\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.2848 - acc: 0.8846\n",
            "processing fold # 4\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 1.4103 - acc: 0.5479\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.9188 - acc: 0.5446\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.7383 - acc: 0.6113\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6807 - acc: 0.6828\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.6305 - acc: 0.7104\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6146 - acc: 0.7079\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.6134 - acc: 0.7193\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5834 - acc: 0.7339\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5960 - acc: 0.7266\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5646 - acc: 0.7516\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5645 - acc: 0.7411\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5762 - acc: 0.7548\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5482 - acc: 0.7584\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5599 - acc: 0.7451\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5525 - acc: 0.7605\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5541 - acc: 0.7381\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5445 - acc: 0.7517\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5352 - acc: 0.7647\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.5358 - acc: 0.7788\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5156 - acc: 0.7822\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5085 - acc: 0.7859\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5460 - acc: 0.7486\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4916 - acc: 0.7775\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4918 - acc: 0.7836\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.4990 - acc: 0.7909\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 5s 45ms/step - loss: 0.5095 - acc: 0.7905\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.5106 - acc: 0.7797\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4747 - acc: 0.7962\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4848 - acc: 0.7966\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4829 - acc: 0.7967\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4699 - acc: 0.8122\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4914 - acc: 0.7894\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4933 - acc: 0.7861\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4568 - acc: 0.8188\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4593 - acc: 0.8094\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4560 - acc: 0.8080\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4290 - acc: 0.8334\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4454 - acc: 0.8261\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4386 - acc: 0.8106\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4712 - acc: 0.7999\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4543 - acc: 0.8145\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4555 - acc: 0.8117\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4370 - acc: 0.8224\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4006 - acc: 0.8427\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4470 - acc: 0.8247\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4319 - acc: 0.8212\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4367 - acc: 0.8244\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4391 - acc: 0.8007\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4405 - acc: 0.8168\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4209 - acc: 0.8247\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4367 - acc: 0.8118\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4130 - acc: 0.8328\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4363 - acc: 0.8162\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4276 - acc: 0.8278\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4171 - acc: 0.8354\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3951 - acc: 0.8389\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4171 - acc: 0.8313\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4030 - acc: 0.8373\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4472 - acc: 0.8090\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4163 - acc: 0.8290\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4090 - acc: 0.8402\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.4216 - acc: 0.8285\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4079 - acc: 0.8257\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4111 - acc: 0.8380\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4054 - acc: 0.8483\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4064 - acc: 0.8229\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3968 - acc: 0.8439\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 5s 44ms/step - loss: 0.3868 - acc: 0.8413\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4216 - acc: 0.8268\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4219 - acc: 0.8268\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4166 - acc: 0.8277\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3563 - acc: 0.8618\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3952 - acc: 0.8381\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4088 - acc: 0.8358\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4091 - acc: 0.8326\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3842 - acc: 0.8573\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3936 - acc: 0.8413\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3980 - acc: 0.8444\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 5s 42ms/step - loss: 0.3936 - acc: 0.8452\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3763 - acc: 0.8436\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3931 - acc: 0.8447\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4036 - acc: 0.8420\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3797 - acc: 0.8403\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4013 - acc: 0.8408\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3971 - acc: 0.8350\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3932 - acc: 0.8519\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3887 - acc: 0.8404\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3913 - acc: 0.8357\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3798 - acc: 0.8386\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3721 - acc: 0.8524\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3786 - acc: 0.8449\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.4064 - acc: 0.8353\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3659 - acc: 0.8520\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3615 - acc: 0.8493\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3606 - acc: 0.8498\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3771 - acc: 0.8410\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3764 - acc: 0.8396\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3817 - acc: 0.8348\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3739 - acc: 0.8557\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 5s 43ms/step - loss: 0.3646 - acc: 0.8513\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3127 - acc: 0.8731\n",
            "-----------------------------------------------------\n",
            "batch_size: \t 32\n",
            "num_epochs: \t 100\n",
            "units_per_layer: [32  64  128  128]\n",
            "optimizer: \t RMSprop\n",
            "learning_rate: \t 0.01\n",
            "num_folds CV: \t 5\n",
            "-----------------------------------------------------\n",
            "processing fold # 0\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 6s 69ms/step - loss: 1.4776 - acc: 0.5426\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 1.0866 - acc: 0.5676\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.8680 - acc: 0.5653\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.7553 - acc: 0.6104\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.6896 - acc: 0.6851\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6401 - acc: 0.7290\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6049 - acc: 0.7421\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6098 - acc: 0.7308\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6065 - acc: 0.7357\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.6000 - acc: 0.7228\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5830 - acc: 0.7357\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5708 - acc: 0.7392\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5738 - acc: 0.7509\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5867 - acc: 0.7278\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5711 - acc: 0.7548\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5661 - acc: 0.7346\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5454 - acc: 0.7574\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5364 - acc: 0.7759\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5662 - acc: 0.7380\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5414 - acc: 0.7687\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5430 - acc: 0.7503\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5534 - acc: 0.7560\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5377 - acc: 0.7585\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5285 - acc: 0.7846\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5311 - acc: 0.7603\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5318 - acc: 0.7639\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5212 - acc: 0.7778\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5222 - acc: 0.7631\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5127 - acc: 0.7746\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5247 - acc: 0.7627\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5205 - acc: 0.7767\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5039 - acc: 0.7807\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4855 - acc: 0.7899\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4892 - acc: 0.7826\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4732 - acc: 0.7969\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4844 - acc: 0.8027\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5016 - acc: 0.7818\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4844 - acc: 0.8002\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4795 - acc: 0.7969\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4939 - acc: 0.7860\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4731 - acc: 0.8073\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4670 - acc: 0.8214\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4551 - acc: 0.8177\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4518 - acc: 0.8351\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4609 - acc: 0.7978\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4637 - acc: 0.8113\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4510 - acc: 0.8055\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4548 - acc: 0.8037\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4537 - acc: 0.8184\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4541 - acc: 0.8161\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4484 - acc: 0.8242\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4446 - acc: 0.8112\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4388 - acc: 0.8265\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4440 - acc: 0.8124\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4037 - acc: 0.8304\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4657 - acc: 0.8051\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3844 - acc: 0.8596\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4601 - acc: 0.8289\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4145 - acc: 0.8327\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4350 - acc: 0.8155\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4496 - acc: 0.8091\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4512 - acc: 0.8163\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4125 - acc: 0.8505\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4199 - acc: 0.8240\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4291 - acc: 0.8251\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4124 - acc: 0.8241\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4170 - acc: 0.8369\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4370 - acc: 0.8178\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - 5s 72ms/step - loss: 0.4168 - acc: 0.8223\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - 5s 71ms/step - loss: 0.4292 - acc: 0.8215\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4200 - acc: 0.8366\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4218 - acc: 0.8222\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4110 - acc: 0.8260\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3993 - acc: 0.8364\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4162 - acc: 0.8382\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4081 - acc: 0.8452\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4194 - acc: 0.8377\n",
            "Epoch 78/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4245 - acc: 0.8296\n",
            "Epoch 79/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4164 - acc: 0.8279\n",
            "Epoch 80/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4117 - acc: 0.8398\n",
            "Epoch 81/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4007 - acc: 0.8416\n",
            "Epoch 82/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3853 - acc: 0.8479\n",
            "Epoch 83/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4116 - acc: 0.8317\n",
            "Epoch 84/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3867 - acc: 0.8455\n",
            "Epoch 85/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3906 - acc: 0.8422\n",
            "Epoch 86/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3912 - acc: 0.8381\n",
            "Epoch 87/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3971 - acc: 0.8419\n",
            "Epoch 88/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3895 - acc: 0.8455\n",
            "Epoch 89/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3949 - acc: 0.8435\n",
            "Epoch 90/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3903 - acc: 0.8428\n",
            "Epoch 91/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3919 - acc: 0.8552\n",
            "Epoch 92/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3867 - acc: 0.8432\n",
            "Epoch 93/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3624 - acc: 0.8510\n",
            "Epoch 94/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3958 - acc: 0.8409\n",
            "Epoch 95/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3890 - acc: 0.8368\n",
            "Epoch 96/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3951 - acc: 0.8392\n",
            "Epoch 97/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3823 - acc: 0.8556\n",
            "Epoch 98/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3661 - acc: 0.8458\n",
            "Epoch 99/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3815 - acc: 0.8471\n",
            "Epoch 100/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4019 - acc: 0.8312\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3201 - acc: 0.8691\n",
            "processing fold # 1\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 1.4610 - acc: 0.5451\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 1.0668 - acc: 0.5778\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.8471 - acc: 0.6038\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.7544 - acc: 0.6400\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6900 - acc: 0.6685\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.6795 - acc: 0.6654\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6390 - acc: 0.7138\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.6141 - acc: 0.7441\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6218 - acc: 0.7157\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5836 - acc: 0.7492\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5854 - acc: 0.7392\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5816 - acc: 0.7409\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5741 - acc: 0.7544\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5903 - acc: 0.7388\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5638 - acc: 0.7516\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5727 - acc: 0.7436\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5631 - acc: 0.7418\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5745 - acc: 0.7306\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 5s 71ms/step - loss: 0.5455 - acc: 0.7593\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5395 - acc: 0.7585\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5592 - acc: 0.7619\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5652 - acc: 0.7419\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5285 - acc: 0.7640\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5651 - acc: 0.7460\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5310 - acc: 0.7661\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5165 - acc: 0.7647\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5165 - acc: 0.7758\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5154 - acc: 0.7702\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5022 - acc: 0.7992\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5282 - acc: 0.7809\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4919 - acc: 0.7840\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4764 - acc: 0.8070\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4961 - acc: 0.7792\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 5s 71ms/step - loss: 0.4523 - acc: 0.8258\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 5s 71ms/step - loss: 0.4859 - acc: 0.7886\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4777 - acc: 0.8058\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4921 - acc: 0.7885\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4803 - acc: 0.8076\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4794 - acc: 0.7990\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4918 - acc: 0.7872\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4971 - acc: 0.7724\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4475 - acc: 0.8124\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4341 - acc: 0.8206\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4795 - acc: 0.8014\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4526 - acc: 0.8087\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4600 - acc: 0.8156\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4468 - acc: 0.8185\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4122 - acc: 0.8426\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4684 - acc: 0.8067\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4452 - acc: 0.8028\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4616 - acc: 0.8177\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4391 - acc: 0.8212\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4289 - acc: 0.8273\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4183 - acc: 0.8283\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4548 - acc: 0.8114\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4143 - acc: 0.8286\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4474 - acc: 0.8115\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4085 - acc: 0.8319\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4131 - acc: 0.8363\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4152 - acc: 0.8254\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4208 - acc: 0.8288\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4101 - acc: 0.8344\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4247 - acc: 0.8324\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4163 - acc: 0.8368\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4036 - acc: 0.8390\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4001 - acc: 0.8336\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4064 - acc: 0.8329\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4010 - acc: 0.8315\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4059 - acc: 0.8231\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4137 - acc: 0.8270\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4173 - acc: 0.8311\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4093 - acc: 0.8339\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3857 - acc: 0.8460\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3852 - acc: 0.8508\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4236 - acc: 0.8309\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3824 - acc: 0.8483\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4047 - acc: 0.8377\n",
            "Epoch 78/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3786 - acc: 0.8502\n",
            "Epoch 79/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4098 - acc: 0.8215\n",
            "Epoch 80/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4037 - acc: 0.8420\n",
            "Epoch 81/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4401 - acc: 0.8252\n",
            "Epoch 82/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3678 - acc: 0.8476\n",
            "Epoch 83/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3727 - acc: 0.8572\n",
            "Epoch 84/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4041 - acc: 0.8341\n",
            "Epoch 85/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3777 - acc: 0.8467\n",
            "Epoch 86/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3767 - acc: 0.8425\n",
            "Epoch 87/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3853 - acc: 0.8379\n",
            "Epoch 88/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3770 - acc: 0.8531\n",
            "Epoch 89/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4022 - acc: 0.8347\n",
            "Epoch 90/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3790 - acc: 0.8444\n",
            "Epoch 91/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3636 - acc: 0.8466\n",
            "Epoch 92/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3935 - acc: 0.8369\n",
            "Epoch 93/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3694 - acc: 0.8549\n",
            "Epoch 94/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3878 - acc: 0.8296\n",
            "Epoch 95/100\n",
            "66/66 [==============================] - 5s 71ms/step - loss: 0.3734 - acc: 0.8456\n",
            "Epoch 96/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3853 - acc: 0.8429\n",
            "Epoch 97/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3932 - acc: 0.8421\n",
            "Epoch 98/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3782 - acc: 0.8388\n",
            "Epoch 99/100\n",
            "66/66 [==============================] - 5s 71ms/step - loss: 0.3556 - acc: 0.8483\n",
            "Epoch 100/100\n",
            "66/66 [==============================] - 5s 71ms/step - loss: 0.3529 - acc: 0.8499\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.3300 - acc: 0.8750\n",
            "processing fold # 2\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 1.4814 - acc: 0.5027\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 1.0946 - acc: 0.5576\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.8691 - acc: 0.5823\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.7641 - acc: 0.6099\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.7052 - acc: 0.6720\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6590 - acc: 0.7075\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6479 - acc: 0.7173\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6154 - acc: 0.7253\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6153 - acc: 0.7371\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.6241 - acc: 0.7100\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 5s 71ms/step - loss: 0.5856 - acc: 0.7526\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5848 - acc: 0.7524\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5706 - acc: 0.7557\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5947 - acc: 0.7393\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5677 - acc: 0.7499\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5624 - acc: 0.7376\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5540 - acc: 0.7660\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5524 - acc: 0.7652\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5572 - acc: 0.7450\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5515 - acc: 0.7563\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5568 - acc: 0.7587\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5252 - acc: 0.7650\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5497 - acc: 0.7689\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5354 - acc: 0.7666\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5435 - acc: 0.7532\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5370 - acc: 0.7671\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5394 - acc: 0.7674\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5218 - acc: 0.7623\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5193 - acc: 0.7752\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4956 - acc: 0.7854\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5075 - acc: 0.7832\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4874 - acc: 0.7993\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5141 - acc: 0.7646\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4970 - acc: 0.7900\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4795 - acc: 0.7935\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4844 - acc: 0.7976\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5124 - acc: 0.7864\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4900 - acc: 0.7837\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5105 - acc: 0.7873\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4693 - acc: 0.7994\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4792 - acc: 0.7925\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4919 - acc: 0.7883\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5014 - acc: 0.7702\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4778 - acc: 0.8076\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4647 - acc: 0.8117\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4851 - acc: 0.7936\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4562 - acc: 0.7892\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4811 - acc: 0.8019\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4718 - acc: 0.8003\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4709 - acc: 0.8035\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4575 - acc: 0.8016\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4560 - acc: 0.8148\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4668 - acc: 0.8012\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4662 - acc: 0.8025\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4422 - acc: 0.8312\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4510 - acc: 0.8089\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4150 - acc: 0.8375\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4418 - acc: 0.8100\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4461 - acc: 0.8187\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4422 - acc: 0.8266\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4246 - acc: 0.8174\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4580 - acc: 0.8211\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4439 - acc: 0.8153\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4235 - acc: 0.8152\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4285 - acc: 0.8331\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4407 - acc: 0.8104\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4533 - acc: 0.8133\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4316 - acc: 0.8206\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4271 - acc: 0.8155\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.3933 - acc: 0.8426\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4237 - acc: 0.8307\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4172 - acc: 0.8362\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4284 - acc: 0.8265\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4086 - acc: 0.8401\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4327 - acc: 0.8256\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4189 - acc: 0.8219\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4063 - acc: 0.8351\n",
            "Epoch 78/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4063 - acc: 0.8420\n",
            "Epoch 79/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4159 - acc: 0.8292\n",
            "Epoch 80/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4181 - acc: 0.8258\n",
            "Epoch 81/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4155 - acc: 0.8382\n",
            "Epoch 82/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4089 - acc: 0.8469\n",
            "Epoch 83/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4093 - acc: 0.8447\n",
            "Epoch 84/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3993 - acc: 0.8332\n",
            "Epoch 85/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4234 - acc: 0.8174\n",
            "Epoch 86/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4119 - acc: 0.8394\n",
            "Epoch 87/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4105 - acc: 0.8428\n",
            "Epoch 88/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3865 - acc: 0.8433\n",
            "Epoch 89/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4235 - acc: 0.8252\n",
            "Epoch 90/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4134 - acc: 0.8377\n",
            "Epoch 91/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4137 - acc: 0.8344\n",
            "Epoch 92/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4195 - acc: 0.8348\n",
            "Epoch 93/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3849 - acc: 0.8526\n",
            "Epoch 94/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4079 - acc: 0.8510\n",
            "Epoch 95/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3973 - acc: 0.8418\n",
            "Epoch 96/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3903 - acc: 0.8398\n",
            "Epoch 97/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4172 - acc: 0.8223\n",
            "Epoch 98/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.3941 - acc: 0.8415\n",
            "Epoch 99/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3914 - acc: 0.8435\n",
            "Epoch 100/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3984 - acc: 0.8375\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3292 - acc: 0.8711\n",
            "processing fold # 3\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 1.4799 - acc: 0.5468\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 1.0985 - acc: 0.5490\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.8789 - acc: 0.5676\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.7713 - acc: 0.6147\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.7144 - acc: 0.6485\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.6608 - acc: 0.6914\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6463 - acc: 0.7116\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6306 - acc: 0.7327\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6412 - acc: 0.7274\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6124 - acc: 0.7242\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6012 - acc: 0.7210\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5813 - acc: 0.7458\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5629 - acc: 0.7448\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5684 - acc: 0.7530\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5413 - acc: 0.7751\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5633 - acc: 0.7642\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5681 - acc: 0.7579\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5612 - acc: 0.7541\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5511 - acc: 0.7628\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5426 - acc: 0.7676\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5898 - acc: 0.7208\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.5307 - acc: 0.7795\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5470 - acc: 0.7400\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5354 - acc: 0.7474\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5344 - acc: 0.7574\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5077 - acc: 0.7786\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5229 - acc: 0.7564\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5155 - acc: 0.7787\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5237 - acc: 0.7729\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5177 - acc: 0.7654\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5101 - acc: 0.7766\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5184 - acc: 0.7756\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5262 - acc: 0.7766\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4940 - acc: 0.7814\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4852 - acc: 0.7927\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5118 - acc: 0.7718\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5187 - acc: 0.7593\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5027 - acc: 0.7808\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4938 - acc: 0.7928\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4997 - acc: 0.7873\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5203 - acc: 0.7762\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5033 - acc: 0.7721\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4561 - acc: 0.8069\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4770 - acc: 0.8159\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4769 - acc: 0.7920\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4646 - acc: 0.8126\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4838 - acc: 0.7932\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4708 - acc: 0.7969\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4607 - acc: 0.8052\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4732 - acc: 0.7992\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4785 - acc: 0.7685\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4661 - acc: 0.7942\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4826 - acc: 0.7876\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4692 - acc: 0.8011\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4428 - acc: 0.8099\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4717 - acc: 0.8116\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4506 - acc: 0.8061\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4649 - acc: 0.7974\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4613 - acc: 0.7985\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4603 - acc: 0.7926\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4644 - acc: 0.8083\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4428 - acc: 0.8126\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4412 - acc: 0.8231\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4418 - acc: 0.8124\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4614 - acc: 0.8069\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4374 - acc: 0.8403\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4297 - acc: 0.8228\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4307 - acc: 0.8293\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4433 - acc: 0.8261\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4165 - acc: 0.8287\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4605 - acc: 0.8006\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4286 - acc: 0.8273\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4435 - acc: 0.8150\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4435 - acc: 0.8103\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4401 - acc: 0.8122\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4137 - acc: 0.8288\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4423 - acc: 0.8088\n",
            "Epoch 78/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4246 - acc: 0.8226\n",
            "Epoch 79/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4421 - acc: 0.8179\n",
            "Epoch 80/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4157 - acc: 0.8322\n",
            "Epoch 81/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4170 - acc: 0.8147\n",
            "Epoch 82/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4237 - acc: 0.8356\n",
            "Epoch 83/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4241 - acc: 0.8307\n",
            "Epoch 84/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4105 - acc: 0.8284\n",
            "Epoch 85/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4168 - acc: 0.8183\n",
            "Epoch 86/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.3971 - acc: 0.8417\n",
            "Epoch 87/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.3984 - acc: 0.8446\n",
            "Epoch 88/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4282 - acc: 0.8228\n",
            "Epoch 89/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4054 - acc: 0.8291\n",
            "Epoch 90/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4121 - acc: 0.8320\n",
            "Epoch 91/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4170 - acc: 0.8441\n",
            "Epoch 92/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3826 - acc: 0.8505\n",
            "Epoch 93/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.3795 - acc: 0.8500\n",
            "Epoch 94/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4092 - acc: 0.8352\n",
            "Epoch 95/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4220 - acc: 0.8295\n",
            "Epoch 96/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4002 - acc: 0.8362\n",
            "Epoch 97/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3916 - acc: 0.8366\n",
            "Epoch 98/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.3832 - acc: 0.8422\n",
            "Epoch 99/100\n",
            "66/66 [==============================] - 5s 70ms/step - loss: 0.4093 - acc: 0.8298\n",
            "Epoch 100/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4047 - acc: 0.8235\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3260 - acc: 0.8789\n",
            "processing fold # 4\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 1.4738 - acc: 0.5112\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 1.0857 - acc: 0.5456\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.8670 - acc: 0.5912\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.7505 - acc: 0.6280\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.7084 - acc: 0.6797\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6598 - acc: 0.7026\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6266 - acc: 0.7358\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.6207 - acc: 0.7397\n",
            "Epoch 9/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.6032 - acc: 0.7520\n",
            "Epoch 10/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.6208 - acc: 0.7336\n",
            "Epoch 11/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5870 - acc: 0.7421\n",
            "Epoch 12/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5878 - acc: 0.7463\n",
            "Epoch 13/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5786 - acc: 0.7559\n",
            "Epoch 14/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5674 - acc: 0.7524\n",
            "Epoch 15/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5658 - acc: 0.7478\n",
            "Epoch 16/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5537 - acc: 0.7720\n",
            "Epoch 17/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5774 - acc: 0.7313\n",
            "Epoch 18/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5532 - acc: 0.7729\n",
            "Epoch 19/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5511 - acc: 0.7501\n",
            "Epoch 20/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5284 - acc: 0.7678\n",
            "Epoch 21/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5476 - acc: 0.7594\n",
            "Epoch 22/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5365 - acc: 0.7720\n",
            "Epoch 23/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5280 - acc: 0.7818\n",
            "Epoch 24/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5319 - acc: 0.7732\n",
            "Epoch 25/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.5092 - acc: 0.7840\n",
            "Epoch 26/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5072 - acc: 0.7908\n",
            "Epoch 27/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5207 - acc: 0.7782\n",
            "Epoch 28/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5069 - acc: 0.7825\n",
            "Epoch 29/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5105 - acc: 0.7714\n",
            "Epoch 30/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4971 - acc: 0.7895\n",
            "Epoch 31/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4837 - acc: 0.7978\n",
            "Epoch 32/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.5014 - acc: 0.7903\n",
            "Epoch 33/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.5026 - acc: 0.7848\n",
            "Epoch 34/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4941 - acc: 0.7976\n",
            "Epoch 35/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5093 - acc: 0.7850\n",
            "Epoch 36/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4849 - acc: 0.8100\n",
            "Epoch 37/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4854 - acc: 0.7854\n",
            "Epoch 38/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.5040 - acc: 0.7761\n",
            "Epoch 39/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4832 - acc: 0.7832\n",
            "Epoch 40/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4723 - acc: 0.7953\n",
            "Epoch 41/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4747 - acc: 0.7877\n",
            "Epoch 42/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4907 - acc: 0.7971\n",
            "Epoch 43/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4690 - acc: 0.8039\n",
            "Epoch 44/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4739 - acc: 0.7974\n",
            "Epoch 45/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4603 - acc: 0.7916\n",
            "Epoch 46/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4815 - acc: 0.8038\n",
            "Epoch 47/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4694 - acc: 0.7996\n",
            "Epoch 48/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4707 - acc: 0.8130\n",
            "Epoch 49/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4808 - acc: 0.8020\n",
            "Epoch 50/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4489 - acc: 0.8074\n",
            "Epoch 51/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4455 - acc: 0.8240\n",
            "Epoch 52/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4586 - acc: 0.8115\n",
            "Epoch 53/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4339 - acc: 0.8305\n",
            "Epoch 54/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4423 - acc: 0.8138\n",
            "Epoch 55/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4705 - acc: 0.7893\n",
            "Epoch 56/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4488 - acc: 0.8276\n",
            "Epoch 57/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4550 - acc: 0.8248\n",
            "Epoch 58/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4444 - acc: 0.8108\n",
            "Epoch 59/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4204 - acc: 0.8315\n",
            "Epoch 60/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4438 - acc: 0.8152\n",
            "Epoch 61/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4581 - acc: 0.8203\n",
            "Epoch 62/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4541 - acc: 0.8014\n",
            "Epoch 63/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4516 - acc: 0.8158\n",
            "Epoch 64/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4361 - acc: 0.8149\n",
            "Epoch 65/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4374 - acc: 0.8155\n",
            "Epoch 66/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4549 - acc: 0.8177\n",
            "Epoch 67/100\n",
            "66/66 [==============================] - 4s 66ms/step - loss: 0.4305 - acc: 0.8176\n",
            "Epoch 68/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4158 - acc: 0.8255\n",
            "Epoch 69/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4443 - acc: 0.8110\n",
            "Epoch 70/100\n",
            "66/66 [==============================] - 4s 66ms/step - loss: 0.4450 - acc: 0.8307\n",
            "Epoch 71/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4150 - acc: 0.8349\n",
            "Epoch 72/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.4127 - acc: 0.8420\n",
            "Epoch 73/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4384 - acc: 0.8105\n",
            "Epoch 74/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4223 - acc: 0.8290\n",
            "Epoch 75/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4279 - acc: 0.8181\n",
            "Epoch 76/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4015 - acc: 0.8488\n",
            "Epoch 77/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4192 - acc: 0.8404\n",
            "Epoch 78/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4338 - acc: 0.8306\n",
            "Epoch 79/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4017 - acc: 0.8481\n",
            "Epoch 80/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4013 - acc: 0.8436\n",
            "Epoch 81/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4332 - acc: 0.8204\n",
            "Epoch 82/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4182 - acc: 0.8310\n",
            "Epoch 83/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.3802 - acc: 0.8435\n",
            "Epoch 84/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.3920 - acc: 0.8411\n",
            "Epoch 85/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4088 - acc: 0.8251\n",
            "Epoch 86/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4229 - acc: 0.8479\n",
            "Epoch 87/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.3870 - acc: 0.8500\n",
            "Epoch 88/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.3912 - acc: 0.8441\n",
            "Epoch 89/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4110 - acc: 0.8308\n",
            "Epoch 90/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4136 - acc: 0.8309\n",
            "Epoch 91/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.4011 - acc: 0.8518\n",
            "Epoch 92/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.4024 - acc: 0.8536\n",
            "Epoch 93/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.3996 - acc: 0.8384\n",
            "Epoch 94/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.4005 - acc: 0.8386\n",
            "Epoch 95/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 0.3887 - acc: 0.8453\n",
            "Epoch 96/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.3734 - acc: 0.8537\n",
            "Epoch 97/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.3986 - acc: 0.8414\n",
            "Epoch 98/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.3951 - acc: 0.8367\n",
            "Epoch 99/100\n",
            "66/66 [==============================] - 4s 68ms/step - loss: 0.3917 - acc: 0.8555\n",
            "Epoch 100/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.3761 - acc: 0.8467\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3085 - acc: 0.8848\n",
            "-----------------------------------------------------\n",
            "batch_size: \t 32\n",
            "num_epochs: \t 100\n",
            "units_per_layer: [32  64  128  256]\n",
            "optimizer: \t RMSprop\n",
            "learning_rate: \t 0.01\n",
            "num_folds CV: \t 5\n",
            "-----------------------------------------------------\n",
            "processing fold # 0\n",
            "Epoch 1/100\n",
            "66/66 [==============================] - 5s 67ms/step - loss: 1.4707 - acc: 0.5293\n",
            "Epoch 2/100\n",
            "66/66 [==============================] - 5s 69ms/step - loss: 1.0764 - acc: 0.5472\n",
            "Epoch 3/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.8607 - acc: 0.5482\n",
            "Epoch 4/100\n",
            "66/66 [==============================] - 5s 68ms/step - loss: 0.7464 - acc: 0.5968\n",
            "Epoch 5/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.6944 - acc: 0.6475\n",
            "Epoch 6/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.6650 - acc: 0.6622\n",
            "Epoch 7/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.6378 - acc: 0.7175\n",
            "Epoch 8/100\n",
            "66/66 [==============================] - 4s 67ms/step - loss: 0.6062 - acc: 0.7384\n",
            "Epoch 9/100\n",
            "50/66 [=====================>........] - ETA: 1s - loss: 0.5897 - acc: 0.7254"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-cbd6335998e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCNN_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits_per_layer_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_regularization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-a6a980dc1cbb>\u001b[0m in \u001b[0;36mCNN_tuning\u001b[0;34m(num_folds, batch_sizes, num_epochs, layers_number, units_per_layer_dict, learning_rates, dropout_reg, dataset, targets)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#cross validate CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#save results on csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-8ab61cb6e97e>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(k, batch_size, num_epochs, dataset, targets, verbosity)\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_train_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                         callbacks=[GarbageCollectorCallback()])\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     val_loss, val_acc = model.evaluate(valid_datagen.flow(validation_data,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheVoIOTYxfa"
      },
      "source": [
        "##Testing best k models: \r\n",
        "feeding the models with all available data and evaluating these one last time on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VKyXizMIfAT",
        "outputId": "09185c16-e00c-441f-e8dd-04393deda014"
      },
      "source": [
        "# Full training set non-splitted \r\n",
        "print(train_images.shape)\r\n",
        "print(train_labels.shape)\r\n",
        "print(test_images.shape)\r\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2676, 150, 150, 1)\n",
            "(2676,)\n",
            "(336, 150, 150, 1)\n",
            "(336,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PFZGbUeZPI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbfb6d2-d56c-434c-e74f-81e35d127a74"
      },
      "source": [
        "def evaluate_best_model(k, file_path, file_path_out):\r\n",
        "  # Load results  \r\n",
        "  data = pd.read_csv(file_path)\r\n",
        "\r\n",
        "  # Sort results by accuracies \r\n",
        "  data.sort_values(by=['mean_val_acc'], ascending=False, inplace=True)\r\n",
        "  data.head()\r\n",
        "\r\n",
        "  # Init variables\r\n",
        "  layers_number = 4\r\n",
        "  batch_size = 0\r\n",
        "  num_epochs = 0\r\n",
        "  units_per_layer = []\r\n",
        "  optimizer = 0\r\n",
        "  learning_rate = 0\r\n",
        "  dropout=True\r\n",
        "\r\n",
        "  with open(file_path_out, 'w') as f:\r\n",
        "    header = \"batch_size,num_epochs,units_per_layer,optimizer,learning_rate,mean_val_acc,mean_val_loss,prediction_acc\\n\"\r\n",
        "    f.write(header)\r\n",
        "\r\n",
        "  # Get parameters values\r\n",
        "  for index, row in data[:k].iterrows():\r\n",
        "\r\n",
        "    #print info \r\n",
        "    print(\"-----------------------------------------------------\")\r\n",
        "    print(\"batch_size: \\t\", str(row[\"batch_size\"]))\r\n",
        "    print(\"num_epochs: \\t\", str(row[\"num_epochs\"]))\r\n",
        "    print(\"units_per_layer:\", str(row[\"units_per_layer\"]))\r\n",
        "    print(\"optimizer: \\t\", str(row[\"optimizer\"]))\r\n",
        "    print(\"learning_rate: \\t\", str(row[\"learning_rate\"]))\r\n",
        "    print(\"mean_val_acc: \\t\", str(row[\"mean_val_acc\"]))\r\n",
        "    print(\"mean_val_loss: \\t\", str(row[\"mean_val_loss\"]))\r\n",
        "    print(\"-----------------------------------------------------\")\r\n",
        "\r\n",
        "\r\n",
        "    batch_size = int(row[\"batch_size\"])\r\n",
        "    num_epochs = int(row[\"num_epochs\"])\r\n",
        "    units_per_layer = ast.literal_eval(row[\"units_per_layer\"].replace(\"  \", \",\"))\r\n",
        "    optimizer = row[\"optimizer\"]\r\n",
        "    learning_rate = row[\"learning_rate\"]\r\n",
        "\r\n",
        "    if optimizer == \"Adam\":\r\n",
        "      opt = optimizers.Adam(learning_rate=float(learning_rate))\r\n",
        "    else: \r\n",
        "      opt = optimizers.RMSprop(learning_rate=float(learning_rate))\r\n",
        "\r\n",
        "    # Build model\r\n",
        "    model = build_custom_model(layers_number, units_per_layer, batch_size, dropout, opt)\r\n",
        "\r\n",
        "    # Fit model on all the available data\r\n",
        "    history = model.fit(train_datagen.flow(train_images, \r\n",
        "                                          train_labels,\r\n",
        "                                          batch_size=batch_size,\r\n",
        "                                          shuffle=False),\r\n",
        "                        epochs=num_epochs,\r\n",
        "                        steps_per_epoch=len(train_images) // batch_size,\r\n",
        "                        callbacks=[GarbageCollectorCallback()])\r\n",
        "    \r\n",
        "    # Evaluate model on test set \r\n",
        "    test_loss, test_acc = model.evaluate(test_datagen.flow(test_images,\r\n",
        "                                                        test_labels,\r\n",
        "                                                        batch_size=batch_size,\r\n",
        "                                                        shuffle=False),\r\n",
        "                                        steps=len(test_images) // batch_size,\r\n",
        "                                        callbacks=[GarbageCollectorCallback()])\r\n",
        "\r\n",
        "    print(\"Accuracy:\", \"%0.2f\" % (test_acc*100), \"%\")\r\n",
        "\r\n",
        "    # Get the prediction for each sample\r\n",
        "    predictions = model.predict(\r\n",
        "      test_images,\r\n",
        "      max_queue_size=10,\r\n",
        "      callbacks=[GarbageCollectorCallback()]) \r\n",
        "    \r\n",
        "    predicted = np.argmax(predictions,axis=1) \r\n",
        "\r\n",
        "    correct_predictions = np.sum(np.equal(predicted,test_labels))\r\n",
        "    accuracy = correct_predictions/len(test_labels)\r\n",
        "    print(\"\\n-----------------------------------------------------\")\r\n",
        "    print(\"Correct Predictions\")\r\n",
        "    print(\"Accuracy:\", \"%0.2f\" % (accuracy*100), \"%\")\r\n",
        "    print(\"-----------------------------------------------------\\n\")\r\n",
        "\r\n",
        "    if index == 0:\r\n",
        "      model.save(model_path)\r\n",
        "\r\n",
        "    # Save results on csv file \r\n",
        "    with open(file_path_out, 'a') as f:\r\n",
        "      row = str(row[\"batch_size\"]) + \",\" \\\r\n",
        "          + str(row[\"num_epochs\"]) + \",\" \\\r\n",
        "          + str(row[\"units_per_layer\"]) + \",\" \\\r\n",
        "          + str(row[\"optimizer\"]) + \",\" \\\r\n",
        "          + str(row[\"learning_rate\"]) + \",\" \\\r\n",
        "          + str(\"%0.4f\" % (test_acc)) + \",\" \\\r\n",
        "          + str(\"%0.4f\" % (test_loss)) + \",\" \\\r\n",
        "          + str(accuracy) + \"\\n\"\r\n",
        "      f.write(row)\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib4nzirrh6Jn"
      },
      "source": [
        "data[:k].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CT4ff1dw884",
        "outputId": "36b49d0f-87b1-4b7f-eb08-c69229f4fc72"
      },
      "source": [
        "# Get from file the a list of models and evaluate the best\r\n",
        "evaluate_best_model(k, file_path, file_path_out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------\n",
            "batch_size: \t 20\n",
            "num_epochs: \t 100\n",
            "units_per_layer: [32  64  128  256]\n",
            "optimizer: \t RMSprop\n",
            "learning_rate: \t 0.001\n",
            "mean_val_acc: \t 0.8865\n",
            "mean_val_loss: \t 0.3096 (+/- 0.0342)\n",
            "-----------------------------------------------------\n",
            "Epoch 1/100\n",
            "133/133 [==============================] - 12s 38ms/step - loss: 0.7660 - acc: 0.5363\n",
            "Epoch 2/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.6989 - acc: 0.5430\n",
            "Epoch 3/100\n",
            "133/133 [==============================] - 5s 39ms/step - loss: 0.6929 - acc: 0.5282\n",
            "Epoch 4/100\n",
            "133/133 [==============================] - 5s 39ms/step - loss: 0.6911 - acc: 0.5373\n",
            "Epoch 5/100\n",
            "133/133 [==============================] - 5s 39ms/step - loss: 0.6927 - acc: 0.5460\n",
            "Epoch 6/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.6894 - acc: 0.5552\n",
            "Epoch 7/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.6926 - acc: 0.5569\n",
            "Epoch 8/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.6927 - acc: 0.5404\n",
            "Epoch 9/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.6842 - acc: 0.5802\n",
            "Epoch 10/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.6747 - acc: 0.6101\n",
            "Epoch 11/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.6740 - acc: 0.5944\n",
            "Epoch 12/100\n",
            "133/133 [==============================] - 5s 37ms/step - loss: 0.6231 - acc: 0.6795\n",
            "Epoch 13/100\n",
            "133/133 [==============================] - 5s 37ms/step - loss: 0.5975 - acc: 0.6882\n",
            "Epoch 14/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.5776 - acc: 0.7195\n",
            "Epoch 15/100\n",
            "133/133 [==============================] - 5s 37ms/step - loss: 0.5680 - acc: 0.7314\n",
            "Epoch 16/100\n",
            "133/133 [==============================] - 5s 39ms/step - loss: 0.5282 - acc: 0.7550\n",
            "Epoch 17/100\n",
            "133/133 [==============================] - 5s 39ms/step - loss: 0.5168 - acc: 0.7554\n",
            "Epoch 18/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.5022 - acc: 0.7702\n",
            "Epoch 19/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4831 - acc: 0.7889\n",
            "Epoch 20/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4994 - acc: 0.7827\n",
            "Epoch 21/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4519 - acc: 0.8060\n",
            "Epoch 22/100\n",
            "133/133 [==============================] - 5s 37ms/step - loss: 0.4743 - acc: 0.7931\n",
            "Epoch 23/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4305 - acc: 0.8130\n",
            "Epoch 24/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4689 - acc: 0.8184\n",
            "Epoch 25/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4337 - acc: 0.8052\n",
            "Epoch 26/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4390 - acc: 0.7912\n",
            "Epoch 27/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4003 - acc: 0.8263\n",
            "Epoch 28/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4253 - acc: 0.8033\n",
            "Epoch 29/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4286 - acc: 0.8155\n",
            "Epoch 30/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4593 - acc: 0.7881\n",
            "Epoch 31/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4329 - acc: 0.8050\n",
            "Epoch 32/100\n",
            "133/133 [==============================] - 5s 38ms/step - loss: 0.4097 - acc: 0.8261\n",
            "Epoch 33/100\n",
            "133/133 [==============================] - 5s 37ms/step - loss: 0.4235 - acc: 0.8137\n",
            "Epoch 34/100\n",
            "133/133 [==============================] - 5s 39ms/step - loss: 0.4315 - acc: 0.8098\n",
            "Epoch 35/100\n",
            " 13/133 [=>............................] - ETA: 4s - loss: 0.3269 - acc: 0.8673"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "8lKwQPcOrTpO",
        "outputId": "ac048f2c-8600-4941-9d20-371cb97249fd"
      },
      "source": [
        "# Get the prediction for each sample\r\n",
        "model = load_model(model_path)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-04c9cc1bad35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the prediction for each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: /content/gdrive/My Drive/Colab_Notebooks/CIDL/DL Project/numpy data/best_model.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpCFz_ewsJ5k"
      },
      "source": [
        "\r\n",
        "predictions = model.predict(\r\n",
        "  test_images,\r\n",
        "  max_queue_size=10,\r\n",
        "  callbacks=[GarbageCollectorCallback()]) \r\n",
        "\r\n",
        "predicted = np.argmax(predictions,axis=1) \r\n",
        "\r\n",
        "correct_predictions = np.sum(np.equal(predicted,test_labels))\r\n",
        "accuracy = correct_predictions/len(test_labels)\r\n",
        "\r\n",
        "\r\n",
        "# try to load the model again\r\n",
        "\r\n",
        "test_loss, test_acc = model.evaluate(test_images,test_labels)\r\n",
        "\r\n",
        "print('test_acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUOdk75vBJyy"
      },
      "source": [
        "## Reference\r\n",
        "\r\n",
        "\r\n",
        ">**Student** | **Email contact**\r\n",
        ">--- | ---\r\n",
        ">A. Schiavo | a.schiavo2@studenti.unipi.it\r\n",
        ">M. GÃ³mez\t|\tm.gomezgomez@studenti.unipi.it\r\n",
        ">M. Daole |\tm.daole@studenti.unipi.it\r\n"
      ]
    }
  ]
}