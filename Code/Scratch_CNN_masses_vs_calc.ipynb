{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scratch_CNN_masses_vs_calc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarshaGomez/CNN-Medical-Imaging-Analysis/blob/main/Code/Scratch_CNN_masses_vs_calc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPQ8gkXb0NY0"
      },
      "source": [
        "# **Scratch CNN**\r\n",
        "---\r\n",
        "Classification model for discriminating between 2 classes: **masses and calcification**. *Ad-hoc CNN architecture*.\r\n",
        "\r\n",
        "**Students:**   *A. Schiavo - M. Gómez - M. Daole*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB6hExesaEvW"
      },
      "source": [
        "## Data Loading\r\n",
        "This can be easily done with the Python data manipulation. Modern deep learning provides a very powerful framework for supervised learning, we introduce on this step the convolutional network for scaling to large images.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95hM4MvWjwfO"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iduaD281krFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ea9655-ecd4-47ec-f71e-3d68b360c92e"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import ast #Abstract Syntax Trees\r\n",
        "import os \r\n",
        "import gc # Garbage Collector\r\n",
        "\r\n",
        "from tensorflow.keras import backend as K # Useful to free GPU and memory\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from keras import layers\r\n",
        "from keras import optimizers \r\n",
        "from keras import models\r\n",
        "from keras import regularizers\r\n",
        "\r\n",
        "BATCH_SIZE = 20\r\n",
        "EPOCHS = 100\r\n",
        "\r\n",
        "base_path = \"/content/gdrive/My Drive/Colab_Notebooks/CIDL/DL Project\"\r\n",
        "train_img_path = os.path.join(base_path, \"numpy data/train_tensor.npy\")\r\n",
        "train_label_path = os.path.join(base_path, \"numpy data/train_labels.npy\")\r\n",
        "test_img_path = os.path.join(base_path, \"numpy data/public_test_tensor.npy\")\r\n",
        "test_label_path = os.path.join(base_path, \"numpy data/public_test_labels.npy\")\r\n",
        "\r\n",
        "MODEL_PATH = os.path.join(base_path, \"models\")\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HEhhQy4Sr_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d706b6cc-26e5-46ea-f870-4011ad6278e6"
      },
      "source": [
        "# Custom Callback To Include in Callbacks List At Training Time\r\n",
        "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "      gc.collect()\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek9JMlmD6GsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36676b1c-7fcd-4443-cee8-2cee2c743038"
      },
      "source": [
        "# Load Arrays from Numpy Files\r\n",
        "def load_training():\r\n",
        "  train_images = np.load(train_img_path)\r\n",
        "  train_labels = np.load(train_label_path)\r\n",
        "  test_images = np.load(test_img_path)\r\n",
        "  test_labels = np.load(test_label_path)\r\n",
        "\r\n",
        "  return train_images, train_labels, test_images, test_labels\r\n",
        "\r\n",
        "# Remove baseline samples\r\n",
        "def remove_baseline(tensor): \r\n",
        "  max_ind = int(len(tensor)/2)\r\n",
        "  indexes = [2*i + 1 for i in range(0, max_ind)]\r\n",
        "\r\n",
        "  return tensor[indexes]\r\n",
        "\r\n",
        "# Interchange the dataset index\r\n",
        "def shuffle_dataset(x, y):\r\n",
        "  indices = tf.range(start=0, limit=tf.shape(x)[0], dtype=tf.int32)\r\n",
        "  shuffled_indices = tf.random.shuffle(indices)\r\n",
        "\r\n",
        "  x = tf.gather(x, shuffled_indices)\r\n",
        "  y = tf.gather(y, shuffled_indices)\r\n",
        "\r\n",
        "  x = x.numpy()\r\n",
        "  y = y.numpy()\r\n",
        "\r\n",
        "  return x, y\r\n",
        "\r\n",
        "# Unify masses and calcifications \r\n",
        "def labels_mapping(labels):\r\n",
        "  labels_local = np.zeros(shape=labels.shape, dtype=\"float32\")\r\n",
        "  idx = 0\r\n",
        "  for label in labels:\r\n",
        "    # Masses\r\n",
        "    if label == 1 or label == 2:\r\n",
        "      labels_local[idx] = 0\r\n",
        "    # Calcifications\r\n",
        "    else:\r\n",
        "      labels_local[idx] = 1\r\n",
        "    idx += 1\r\n",
        "\r\n",
        "  return labels_local\r\n",
        "\r\n",
        "# Visualization Data Histogram\r\n",
        "def plot(history):\r\n",
        "  acc = history.history['acc']\r\n",
        "  val_acc = history.history['val_acc']\r\n",
        "  loss = history.history['loss']\r\n",
        "  val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "  epochs = range(len(acc))\r\n",
        "\r\n",
        "  plt.plot(epochs, acc, 'bo', label='Training accuracy')\r\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\r\n",
        "  plt.title('Training and validation accuracy')\r\n",
        "  plt.legend()\r\n",
        "\r\n",
        "  plt.figure()\r\n",
        "\r\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\r\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\r\n",
        "  plt.title('Training and validation loss')\r\n",
        "  plt.legend()\r\n",
        "\r\n",
        "  return plt\r\n",
        "\r\n",
        "# Visualization Confusion Matrix\r\n",
        "def plot_confusion_matrix(classes, # Array with the classes name\r\n",
        "                          datagen,\r\n",
        "                          dataset, \r\n",
        "                          labels,\r\n",
        "                          title='Confusion matrix',\r\n",
        "                          cmap=plt.cm.plasma):\r\n",
        "    \"\"\"\r\n",
        "    This function prints and plots the confusion matrix.\r\n",
        "    \"\"\"\r\n",
        "    predicted = model.predict(datagen.flow(dataset,\r\n",
        "                                            labels,\r\n",
        "                                            batch_size=BATCH_SIZE,\r\n",
        "                                            shuffle=False),\r\n",
        "                          steps=len(dataset) // BATCH_SIZE)\r\n",
        "\r\n",
        "    prediction = np.where(predicted < 0.5, 0, 1)\r\n",
        "\r\n",
        "    print('Confusion Matrix')\r\n",
        "    cm = confusion_matrix(labels, prediction)\r\n",
        "\r\n",
        "    print(classification_report(labels, prediction))\r\n",
        "\r\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
        "    plt.title(title)\r\n",
        "    plt.colorbar()\r\n",
        "    tick_marks = np.arange(len(classes))\r\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\r\n",
        "    plt.yticks(tick_marks, classes)\r\n",
        "\r\n",
        "    print(cm)\r\n",
        "\r\n",
        "    thresh = cm.max() / 2.\r\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "        plt.text(j, i, cm[i, j],\r\n",
        "            horizontalalignment=\"center\",\r\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.ylabel('True label')\r\n",
        "    plt.xlabel('Predicted label')\r\n",
        "\r\n",
        "\r\n",
        "# Visualization Detail Metric\r\n",
        "def plot_metrics(history):\r\n",
        "  metrics = ['loss', 'auc', 'precision', 'recall']\r\n",
        "  for n, metric in enumerate(metrics):\r\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\r\n",
        "    plt.subplot(2,2,n+1)\r\n",
        "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\r\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\r\n",
        "             color=colors[0], linestyle=\"--\", label='Val')\r\n",
        "    plt.xlabel('Epoch')\r\n",
        "    plt.ylabel(name)\r\n",
        "    if metric == 'loss':\r\n",
        "      plt.ylim([0, plt.ylim()[1]])\r\n",
        "    elif metric == 'auc':\r\n",
        "      plt.ylim([0.8,1])\r\n",
        "    else:\r\n",
        "      plt.ylim([0,1])\r\n",
        "\r\n",
        "    plt.legend()\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gowt9JUQ6sdj",
        "outputId": "789cc4ee-22e0-46f9-95fe-ed28bbc504cb"
      },
      "source": [
        "# Get images and labels (test, train)\r\n",
        "train_images, train_labels, test_images, test_labels = load_training()\r\n",
        "\r\n",
        "# Get abnormalities only \r\n",
        "train_images = remove_baseline(train_images)\r\n",
        "train_labels = remove_baseline(train_labels)\r\n",
        "test_images = remove_baseline(test_images)\r\n",
        "test_labels = remove_baseline(test_labels)\r\n",
        "\r\n",
        "# Mapping labels with standard index\r\n",
        "train_labels = labels_mapping(train_labels)\r\n",
        "test_labels = labels_mapping(test_labels)\r\n",
        "\r\n",
        "# Suffle index (Previous dataset is ordered by index)\r\n",
        "train_images, train_labels = shuffle_dataset(train_images, train_labels)\r\n",
        "\r\n",
        "print(\"Train shape: \", train_images.shape)\r\n",
        "print(\"Test shape: \", test_images.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape:  (2676, 150, 150)\n",
            "Test shape:  (336, 150, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "nI7cnT2gDyf_",
        "outputId": "3c1597bc-1a5d-4af7-cd1f-8db455db5dbc"
      },
      "source": [
        "# Check the value range and the distribution\r\n",
        "plt.hist(train_images[0]) \r\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrrJFfg2e4fL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd4280b-f57e-4e84-9c0b-ae9e53e35364"
      },
      "source": [
        "# Prepare the data with the expected format\r\n",
        "train_images = train_images.reshape(train_images.shape + (1,))\r\n",
        "test_images = test_images.reshape(test_images.shape + (1,))\r\n",
        "\r\n",
        "print(\"Train shape: \", train_images.shape)\r\n",
        "print(\"Test shape: \", test_images.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape:  (2676, 150, 150, 1)\n",
            "Test shape:  (336, 150, 150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEy_Dc7FhFQN",
        "outputId": "0b5cd8a0-07f2-40a4-8d22-e7a241245713"
      },
      "source": [
        "# Verify values range: \r\n",
        "# Getting max value \r\n",
        "max = max([np.max(image) for image in train_images]) # max is 65535 \r\n",
        "\r\n",
        "# Getting min value\r\n",
        "min = min([np.min(image) for image in train_images]) # min is 0\r\n",
        "\r\n",
        "print(\"Original tensor are of type \", train_images[0].dtype, \" with values in the range [\", min,\",\", max, \"]\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tensor are of type  uint16  with values in the range [ 0 , 65535 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t4iLd0ExZho"
      },
      "source": [
        "## Data preprocessing\r\n",
        "\r\n",
        "Computer vision usually requires relatively little of this kind of preprocessing. The images should be standardized, formatting images to have the same scale is the only kind of preprocessing that is strictly necessary. As optional, we add dataset augmentation because is an excellent way to reduce the generalization error of most computer vision models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfMviUsBn-gX"
      },
      "source": [
        "# Split dataset into training and validation set 70-30\r\n",
        "train_images_split = train_images[:int(0.7*len(train_images))]\r\n",
        "valid_images_split = train_images[int(0.7*len(train_images)):]\r\n",
        "train_labels_split = train_labels[:int(0.7*len(train_labels))]\r\n",
        "valid_labels_split = train_labels[int(0.7*len(train_labels)):]\r\n",
        "\r\n",
        "print(train_images_split.shape)\r\n",
        "print(valid_images_split.shape)                                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm9x-21nzEoE"
      },
      "source": [
        "# All images will be rescaled by 1./65535 (max value range)\r\n",
        "train_datagen = ImageDataGenerator(rescale=1./65535)\r\n",
        "valid_datagen = ImageDataGenerator(rescale=1./65535)\r\n",
        "test_datagen = ImageDataGenerator(rescale=1./65535) \r\n",
        "\r\n",
        "for batch, labels_batch in train_datagen.flow(train_images, train_labels, batch_size=BATCH_SIZE):\r\n",
        "  print(batch.shape)\r\n",
        "  print(labels_batch.shape)\r\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi0ZJVf1vA31"
      },
      "source": [
        "### Defining CNN \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwQTeDPEvKql"
      },
      "source": [
        "def build_model(loss_function, eval_metric):\r\n",
        "  model = models.Sequential()\r\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)))\r\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\r\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "  model.add(layers.Conv2D(128, (3, 3), activation='relu'))\r\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "  model.add(layers.Flatten())\r\n",
        "  #model.add(layers.Dense(512, activation='relu'))\r\n",
        "  #model.add(layers.Dropout(0.5))\r\n",
        "  model.add(layers.Dense(512, kernel_regularizer=regularizers.l2(0.001), activation=\"relu\"))\r\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "  model.compile(loss=loss_function,\r\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4), # lr = 0.0001\r\n",
        "              metrics=[\"acc\"]) \r\n",
        "  \r\n",
        "  return model \r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZIemXxawWLm"
      },
      "source": [
        "### CNN Compilation\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iumGmf1whk1"
      },
      "source": [
        "model = build_model(\"binary_crossentropy\", \"acc\")\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-V8cw96y_3Q"
      },
      "source": [
        "history = model.fit(train_datagen.flow(train_images_split,\r\n",
        "                                       train_labels_split,\r\n",
        "                                       batch_size=BATCH_SIZE,\r\n",
        "                                       shuffle=False),\r\n",
        "                    steps_per_epoch=len(train_images_split) // BATCH_SIZE, \r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    validation_data=valid_datagen.flow(valid_images_split,\r\n",
        "                                       valid_labels_split,\r\n",
        "                                       batch_size=BATCH_SIZE,\r\n",
        "                                       shuffle=False),\r\n",
        "                    validation_steps=len(valid_labels_split) // BATCH_SIZE,\r\n",
        "                    callbacks=[GarbageCollectorCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXMfOZ2Jk6g_",
        "outputId": "820aa278-97dd-4355-ad00-bc53213022bb"
      },
      "source": [
        "# Save Model to drive\r\n",
        "models.save_model(model, os.path.join(MODEL_PATH, 'cnn_compilation.h5'))\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rwzwr-fM3_Y"
      },
      "source": [
        "### Visualizing The Data\r\n",
        "\r\n",
        "One way to do this is by looking at the distribution of some of the dataset’s variables and make scatter plots to see possible correlations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI8FjTjY4KIe"
      },
      "source": [
        "plt = plot(history)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH89us9R_o_F",
        "outputId": "414b29c1-262b-45b4-e94c-95ba29a25905"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_datagen.flow(test_images,\r\n",
        "                                                       test_labels,\r\n",
        "                                                       batch_size=BATCH_SIZE,\r\n",
        "                                                       shuffle=False),\r\n",
        "                                     steps=len(test_images) // BATCH_SIZE,\r\n",
        "                                     callbacks=[GarbageCollectorCallback()])\r\n",
        "\r\n",
        "print(\"Accuracy:\", \"%0.2f\" % (test_acc*100), \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6427 - acc: 0.8406\n",
            "Accuracy: 84.06 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2v_-h75Z3aZ"
      },
      "source": [
        "### Fighting overfitting: 1. Data Augmentation\r\n",
        "\r\n",
        "Neural networks prove not to be very robust to noise, and these plots are characteristic of **overfitting**. Training accuracy keeps increasing linearly while validation accuracy stalls around **82%**. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojvyPceIaunR"
      },
      "source": [
        "# Train data augmentation \r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1./65535,\r\n",
        "    rotation_range=40,\r\n",
        "    width_shift_range=0.2,\r\n",
        "    height_shift_range=0.2,\r\n",
        "    shear_range=20,\r\n",
        "    zoom_range=0.2,\r\n",
        "    horizontal_flip=True,\r\n",
        "    fill_mode='nearest')\r\n",
        "\r\n",
        "valid_datagen = ImageDataGenerator(rescale=1./65535)\r\n",
        "test_datagen = ImageDataGenerator(rescale=1./65535) \r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ex5WyyOwir4"
      },
      "source": [
        "# Building the new model with the train data augmentation\r\n",
        "model = build_model(\"binary_crossentropy\", \"acc\")\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_LVsSUEj0Vx"
      },
      "source": [
        "history = model.fit(train_datagen.flow(train_images_split,\r\n",
        "                                       train_labels_split,\r\n",
        "                                       batch_size=BATCH_SIZE,\r\n",
        "                                       shuffle=False),\r\n",
        "                    steps_per_epoch=len(train_images_split) // BATCH_SIZE, \r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    validation_data=valid_datagen.flow(valid_images_split,\r\n",
        "                                       valid_labels_split,\r\n",
        "                                       batch_size=BATCH_SIZE,\r\n",
        "                                       shuffle=False),\r\n",
        "                    validation_steps=len(valid_labels_split) // BATCH_SIZE,\r\n",
        "                    callbacks=[GarbageCollectorCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNJBV5khoFEA"
      },
      "source": [
        "plt = plot(history)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYDj_8GquRRh"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_datagen.flow(test_images,\r\n",
        "                                                       test_labels,\r\n",
        "                                                       batch_size=BATCH_SIZE,\r\n",
        "                                                       shuffle=False),\r\n",
        "                                     steps=len(test_images) // BATCH_SIZE,\r\n",
        "                                     callbacks=[GarbageCollectorCallback()])\r\n",
        "\r\n",
        "print(\"Accuracy:\", \"%0.2f\" % (test_acc*100), \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrLD4fBGtc6k"
      },
      "source": [
        "##K-fold cross validation.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuiUODCT97Bq"
      },
      "source": [
        "# Train data augmentation for k-fold-cross-validation \r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1./65535,\r\n",
        "    rotation_range=40,\r\n",
        "    width_shift_range=0.2,\r\n",
        "    height_shift_range=0.2,\r\n",
        "    shear_range=20,\r\n",
        "    zoom_range=0.2,\r\n",
        "    horizontal_flip=True,\r\n",
        "    fill_mode='nearest')\r\n",
        "\r\n",
        "valid_datagen = ImageDataGenerator(rescale=1./65535)\r\n",
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgdAZtZ1wMf3"
      },
      "source": [
        "def cross_validate(k, batch_size, num_epochs, dataset, targets, verbosity):\r\n",
        "  #10-Fold-Cross-Validation\r\n",
        "  num_val_samples = len(dataset) // k \r\n",
        "  validation_accuracies = []\r\n",
        "  validation_losses = []\r\n",
        "\r\n",
        "  for i in range(k):\r\n",
        "    # rigen augmented data \r\n",
        "    \r\n",
        "    print(\"processing fold #\", i)\r\n",
        "    validation_data = dataset[i * num_val_samples : (i + 1) * num_val_samples]\r\n",
        "    validation_labels = targets[i * num_val_samples : (i + 1) * num_val_samples]\r\n",
        "\r\n",
        "    partial_train_data = np.concatenate(\r\n",
        "        [dataset[:i * num_val_samples],\r\n",
        "        dataset[(i + 1) * num_val_samples:]], \r\n",
        "        axis=0)\r\n",
        "\r\n",
        "    partial_train_targets = np.concatenate(\r\n",
        "        [targets[:i * num_val_samples],\r\n",
        "        targets[(i + 1) * num_val_samples:]], \r\n",
        "        axis=0)\r\n",
        "\r\n",
        "    model = build_model(\"binary_crossentropy\", \"acc\")\r\n",
        "    \r\n",
        "    history = model.fit(train_datagen.flow(partial_train_data, \r\n",
        "                                          partial_train_targets,\r\n",
        "                                          batch_size=batch_size,\r\n",
        "                                          shuffle=False),\r\n",
        "                        epochs=num_epochs,\r\n",
        "                        steps_per_epoch=len(partial_train_data) // batch_size,\r\n",
        "                        verbose=verbosity,\r\n",
        "                        callbacks=[GarbageCollectorCallback()])\r\n",
        "    \r\n",
        "    val_loss, val_acc = model.evaluate(valid_datagen.flow(validation_data,\r\n",
        "                                                          validation_labels,\r\n",
        "                                                          batch_size=batch_size,\r\n",
        "                                                          shuffle=False),\r\n",
        "                                       steps=len(validation_data) // batch_size,\r\n",
        "                                       callbacks=[GarbageCollectorCallback()])\r\n",
        "    \r\n",
        "    validation_accuracies.append(val_acc)\r\n",
        "    validation_losses.append(val_loss)\r\n",
        "\r\n",
        "  return validation_accuracies, validation_losses \r\n",
        "  \r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdmZ-0dKuKlG"
      },
      "source": [
        "acc, loss = cross_validate(k=10, batch_size=20, num_epochs=100, dataset=train_images, targets=train_labels, verbosity=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3hbM3c6RxTq"
      },
      "source": [
        "print(len(acc))\r\n",
        "print(len(loss))\r\n",
        "print()\r\n",
        "print(np.mean(acc))\r\n",
        "print(np.mean(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQKeyLVTVi7"
      },
      "source": [
        "### Hyperparameters Tuning\r\n",
        "\r\n",
        "To compare the performance of one machine learning algorithm to another, it is necessary to perform controlled experiments\r\n",
        "\r\n",
        "*   Add Dropout or L2 Regularization\r\n",
        "*   Varying of convolutional layers: `[3, 5, 7]`\r\n",
        "*   Change Optimizer (try Adam)\r\n",
        "*   Varying batch size: `[20, 32, 64, 128]`\r\n",
        "*   Varying learning rate\r\n",
        "*   Varying number of units per layer:\r\n",
        "\r\n",
        "```\r\n",
        "Layer  | Unit per Layer \r\n",
        "1      | [32,32,64,128]  \r\n",
        "2      | [32,64,128,128] \r\n",
        "3      | [32,64,128,256] \r\n",
        "4      | [64,64,128,256] \r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO1GEdyfkbRO"
      },
      "source": [
        "#Parameters Grid \r\n",
        "dropout_regularization = True                                               # grafico comparativo: sembra leggermente meglio con Dropout\r\n",
        "batch_sizes = [20, 32, 64, 128]                                             # 4 \r\n",
        "layers_number = 4                                                           # 1\r\n",
        "units_per_layer_dict = [[32,64,128,128], [32,64,128,256]]                   # 2\r\n",
        "learning_rates = [1e-2, 1e-3, 1e-4]                                         # 3\r\n",
        "num_epochs = 100\r\n",
        "num_folds = 5\r\n",
        "\r\n",
        "file_path_out = os.path.join(base_path, \"tuning results/top3_results.csv\")\r\n",
        "file_path = os.path.join(base_path, \"tuning results/results-CM.csv\")\r\n",
        "model_path = os.path.join(base_path, \"tuning results/best_model.h5\")\r\n",
        "\r\n",
        "# Top k models \r\n",
        "k = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQgC-nrCTcn7"
      },
      "source": [
        "def build_custom_model(layers_number, units_per_layer, batch_size, dropout, optimizer):\r\n",
        "  model = models.Sequential()\r\n",
        "\r\n",
        "  for i in range(layers_number):\r\n",
        "     # First layer. Setting input shape\r\n",
        "    if i == 0: \r\n",
        "      model.add(layers.Conv2D(units_per_layer[i], (3, 3), activation='relu', input_shape=(150, 150, 1)))\r\n",
        "      model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "    else:\r\n",
        "      model.add(layers.Conv2D(units_per_layer[i], (3, 3), activation='relu'))\r\n",
        "      model.add(layers.MaxPooling2D((2, 2)))\r\n",
        "\r\n",
        "  model.add(layers.Flatten())\r\n",
        "  model.add(layers.Dense(512, activation='relu'))\r\n",
        "  \r\n",
        "  if dropout:\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "\r\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "  model.compile(loss=\"binary_crossentropy\",\r\n",
        "              optimizer=optimizer,\r\n",
        "              metrics=[\"acc\"]) \r\n",
        "  \r\n",
        "  return model \r\n",
        "\r\n",
        "def CNN_tuning(num_folds, batch_sizes, num_epochs, layers_number, units_per_layer_dict, learning_rates, dropout_reg, dataset, targets):\r\n",
        "  lr = learning_rates[0]\r\n",
        "  opts = [optimizers.RMSprop(learning_rate=lr), optimizers.Adam(learning_rate=lr)]\r\n",
        "\r\n",
        "  with open(file_path, 'w') as f:\r\n",
        "    header = \"batch_size,num_epochs,units_per_layer,optimizer,learning_rate,mean_val_acc,mean_val_loss,num_folds\\n\"\r\n",
        "    f.write(header)\r\n",
        "\r\n",
        "  for batch_size in batch_sizes:                # 4\r\n",
        "    for opt in opts:                            # 2\r\n",
        "      for values_set in units_per_layer_dict:   # 2\r\n",
        "        # Build CNN model \r\n",
        "        model = build_custom_model(layers_number, values_set, batch_size, dropout_reg, opt)\r\n",
        "\r\n",
        "        if \"RMSprop\" in str(opt):\r\n",
        "          str_opt = \"RMSprop\"  \r\n",
        "        else:\r\n",
        "          str_opt = \"Adam\"\r\n",
        "\r\n",
        "        #print info \r\n",
        "        print(\"-----------------------------------------------------\")\r\n",
        "        print(\"batch_size: \\t\", batch_size)\r\n",
        "        print(\"num_epochs: \\t\", num_epochs)\r\n",
        "        print(\"units_per_layer:\", str(values_set).replace(\",\", \" \"))\r\n",
        "        print(\"optimizer: \\t\", str_opt)\r\n",
        "        print(\"learning_rate: \\t\", str(lr))\r\n",
        "        print(\"num_folds CV: \\t\", num_folds)\r\n",
        "        print(\"-----------------------------------------------------\")\r\n",
        "\r\n",
        "        #cross validate CNN model\r\n",
        "        val_acc, val_loss = cross_validate(num_folds, batch_size, num_epochs, dataset, targets, 1)\r\n",
        "\r\n",
        "        #save results on csv file \r\n",
        "        with open(file_path, 'a') as f:\r\n",
        "          row = str(batch_size) + \",\" \\\r\n",
        "              + str(num_epochs) + \",\" \\\r\n",
        "              + str(values_set).replace(\",\", \" \") + \",\" \\\r\n",
        "              + str_opt + \",\" \\\r\n",
        "              + str(lr) + \",\" \\\r\n",
        "              + \"%0.4f (+/- %0.4f)\" % (np.mean(val_acc), np.std(val_acc) * 2) + \",\" \\\r\n",
        "              + \"%0.4f (+/- %0.4f)\" % (np.mean(val_loss), np.std(val_loss) * 2) + \",\" \\\r\n",
        "              + str(num_folds) + \"\\n\"\r\n",
        "          f.write(row)\r\n",
        "\r\n",
        "        del model\r\n",
        "        K.clear_session()\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0fLlSfbPOWB"
      },
      "source": [
        "CNN_tuning(num_folds, batch_sizes, num_epochs, layers_number, units_per_layer_dict, learning_rates, dropout_regularization, train_images, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SheVoIOTYxfa"
      },
      "source": [
        "##Testing best k models: \r\n",
        "feeding the models with all available data and evaluating these one last time on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VKyXizMIfAT"
      },
      "source": [
        "# Full training set non-splitted \r\n",
        "print(train_images.shape)\r\n",
        "print(train_labels.shape)\r\n",
        "print(test_images.shape)\r\n",
        "print(test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PFZGbUeZPI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe52649-4a3b-49a4-aeb3-f026c5a72eec"
      },
      "source": [
        "def evaluate_best_model(k, file_path, file_path_out):\r\n",
        "  # Load results  \r\n",
        "  data = pd.read_csv(file_path)\r\n",
        "\r\n",
        "  # Sort results by accuracies \r\n",
        "  data.sort_values(by=['mean_val_acc'], ascending=False, inplace=True)\r\n",
        "  data.head()\r\n",
        "\r\n",
        "  # Init variables\r\n",
        "  layers_number = 4\r\n",
        "  batch_size = 0\r\n",
        "  num_epochs = 0\r\n",
        "  units_per_layer = []\r\n",
        "  optimizer = 0\r\n",
        "  learning_rate = 0\r\n",
        "  dropout=True\r\n",
        "\r\n",
        "  with open(file_path_out, 'w') as f:\r\n",
        "    header = \"batch_size,num_epochs,units_per_layer,optimizer,learning_rate,mean_val_acc,mean_val_loss,prediction_acc\\n\"\r\n",
        "    f.write(header)\r\n",
        "\r\n",
        "  # Get parameters values\r\n",
        "  for index, row in data[:k].iterrows():\r\n",
        "\r\n",
        "    #print info \r\n",
        "    print(\"-----------------------------------------------------\")\r\n",
        "    print(\"batch_size: \\t\", str(row[\"batch_size\"]))\r\n",
        "    print(\"num_epochs: \\t\", str(row[\"num_epochs\"]))\r\n",
        "    print(\"units_per_layer:\", str(row[\"units_per_layer\"]))\r\n",
        "    print(\"optimizer: \\t\", str(row[\"optimizer\"]))\r\n",
        "    print(\"learning_rate: \\t\", str(row[\"learning_rate\"]))\r\n",
        "    print(\"mean_val_acc: \\t\", str(row[\"mean_val_acc\"]))\r\n",
        "    print(\"mean_val_loss: \\t\", str(row[\"mean_val_loss\"]))\r\n",
        "    print(\"-----------------------------------------------------\")\r\n",
        "\r\n",
        "\r\n",
        "    batch_size = int(row[\"batch_size\"])\r\n",
        "    num_epochs = int(row[\"num_epochs\"])\r\n",
        "    units_per_layer = ast.literal_eval(row[\"units_per_layer\"].replace(\"  \", \",\"))\r\n",
        "    optimizer = row[\"optimizer\"]\r\n",
        "    learning_rate = row[\"learning_rate\"]\r\n",
        "\r\n",
        "    if optimizer == \"Adam\":\r\n",
        "      opt = optimizers.Adam(learning_rate=float(learning_rate))\r\n",
        "    else: \r\n",
        "      opt = optimizers.RMSprop(learning_rate=float(learning_rate))\r\n",
        "\r\n",
        "    # Build model\r\n",
        "    model = build_custom_model(layers_number, units_per_layer, batch_size, dropout, opt)\r\n",
        "\r\n",
        "    # Fit model on all the available data\r\n",
        "    history = model.fit(train_datagen.flow(train_images, \r\n",
        "                                          train_labels,\r\n",
        "                                          batch_size=batch_size,\r\n",
        "                                          shuffle=False),\r\n",
        "                        epochs=num_epochs,\r\n",
        "                        steps_per_epoch=len(train_images) // batch_size,\r\n",
        "                        callbacks=[GarbageCollectorCallback()])\r\n",
        "    \r\n",
        "    # Evaluate model on test set \r\n",
        "    test_loss, test_acc = model.evaluate(test_datagen.flow(test_images,\r\n",
        "                                                        test_labels,\r\n",
        "                                                        batch_size=batch_size,\r\n",
        "                                                        shuffle=False),\r\n",
        "                                        steps=len(test_images) // batch_size,\r\n",
        "                                        callbacks=[GarbageCollectorCallback()])\r\n",
        "\r\n",
        "    print(\"Accuracy:\", \"%0.2f\" % (test_acc*100), \"%\")\r\n",
        "\r\n",
        "    # Get the prediction for each sample\r\n",
        "    predictions = model.predict(\r\n",
        "      test_images,\r\n",
        "      max_queue_size=10,\r\n",
        "      callbacks=[GarbageCollectorCallback()]) \r\n",
        "    \r\n",
        "    predicted = np.argmax(predictions,axis=1) \r\n",
        "\r\n",
        "    correct_predictions = np.sum(np.equal(predicted,test_labels))\r\n",
        "    accuracy = correct_predictions/len(test_labels)\r\n",
        "    print(\"\\n-----------------------------------------------------\")\r\n",
        "    print(\"Correct Predictions\")\r\n",
        "    print(\"Accuracy:\", \"%0.2f\" % (accuracy*100), \"%\")\r\n",
        "    print(\"-----------------------------------------------------\\n\")\r\n",
        "\r\n",
        "    if index == 0:\r\n",
        "      model.save(model_path)\r\n",
        "\r\n",
        "    # Save results on csv file \r\n",
        "    with open(file_path_out, 'a') as f:\r\n",
        "      row = str(row[\"batch_size\"]) + \",\" \\\r\n",
        "          + str(row[\"num_epochs\"]) + \",\" \\\r\n",
        "          + str(row[\"units_per_layer\"]) + \",\" \\\r\n",
        "          + str(row[\"optimizer\"]) + \",\" \\\r\n",
        "          + str(row[\"learning_rate\"]) + \",\" \\\r\n",
        "          + str(\"%0.4f\" % (test_acc)) + \",\" \\\r\n",
        "          + str(\"%0.4f\" % (test_loss)) + \",\" \\\r\n",
        "          + str(accuracy) + \"\\n\"\r\n",
        "      f.write(row)\r\n",
        "\r\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib4nzirrh6Jn"
      },
      "source": [
        "data[:k].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CT4ff1dw884"
      },
      "source": [
        "# Get from file the a list of models and evaluate the best\r\n",
        "evaluate_best_model(k, file_path, file_path_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lKwQPcOrTpO"
      },
      "source": [
        "# Get the prediction for each sample\r\n",
        "model = load_model(model_path)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpCFz_ewsJ5k"
      },
      "source": [
        "\r\n",
        "predictions = model.predict(\r\n",
        "  test_images,\r\n",
        "  max_queue_size=10,\r\n",
        "  callbacks=[GarbageCollectorCallback()]) \r\n",
        "\r\n",
        "predicted = np.argmax(predictions,axis=1) \r\n",
        "\r\n",
        "correct_predictions = np.sum(np.equal(predicted,test_labels))\r\n",
        "accuracy = correct_predictions/len(test_labels)\r\n",
        "\r\n",
        "\r\n",
        "# try to load the model again\r\n",
        "\r\n",
        "test_loss, test_acc = model.evaluate(test_images,test_labels)\r\n",
        "\r\n",
        "print('test_acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmYX-1Ol9cu0"
      },
      "source": [
        "a = [0.111,0.222,1.222]\r\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb1cu__U-Ggj"
      },
      "source": [
        "a = a.astype(int)\r\n",
        "\r\n",
        "np.clip(a, 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yIpdi8y9DZnx",
        "outputId": "855b644a-01c5-493c-fe74-feeb9564dc73"
      },
      "source": [
        "number_list = np.array([1, 1, 3, 3, 2, 3, 4, 4, 1])\r\n",
        "\r\n",
        "(unique, counts) = np.unique(test_labels, return_counts=True)\r\n",
        "\r\n",
        "plt.hist(counts, unique) \r\n",
        "plt.show()\r\n",
        "print(unique, counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 1.] [179 157]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfnUpk9bIwy0"
      },
      "source": [
        "#print confusion matrix\r\n",
        "classes = [\"Masses\", \"Calcification\"]\r\n",
        "plot_confusion_matrix(classes,\r\n",
        "                      test_images,\r\n",
        "                      test_labels,\r\n",
        "                      title='Confusion matrix',\r\n",
        "                      cmap=plt.cm.Blues) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUOdk75vBJyy"
      },
      "source": [
        "## Reference\r\n",
        "\r\n",
        "\r\n",
        ">**Student** | **Email contact**\r\n",
        ">--- | ---\r\n",
        ">A. Schiavo | a.schiavo2@studenti.unipi.it\r\n",
        ">M. Gómez\t|\tm.gomezgomez@studenti.unipi.it\r\n",
        ">M. Daole |\tm.daole@studenti.unipi.it\r\n"
      ]
    }
  ]
}